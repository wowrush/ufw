# dflare_sys.py
"""
D-FLARE SYS â€” å¤šå±¤é›†æˆå¼é˜²ç«ç‰†å¨è„…åˆ†ç´šç³»çµ±ï¼ˆCisco ASA 5506-X å°ˆç”¨ï¼‰
æ”¯æ´æµæ°´è™Ÿï¼ˆbatch_idï¼‰èˆ‡ append æ©Ÿåˆ¶
"""

import os
import sys
import csv
import json
import time
import math  # <--- æ–°å¢
import ipaddress
import pandas as pd
from tqdm import tqdm
from colorama import init, Fore, Style

# ç¹ªåœ–
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

# ç™¾åˆ†æ¯”å››æ¨äº”å…¥ï¼ˆå…©ä½å°æ•¸ï¼‰
from decimal import Decimal, ROUND_HALF_UP

def ceil_to_step(max_val, step=500, minimum=500):
    """
    å°‡ max_val å‘ä¸Šé€²ä½åˆ°æŒ‡å®š step çš„å€æ•¸ï¼ˆé è¨­ 500ï¼‰ï¼Œæœ€å°ä¸Šé™è‡³å°‘ç‚º minimumã€‚
    ä¾‹å¦‚ï¼š3765->4000ã€1234->1500ã€0 æˆ–è² æ•¸ -> 500
    """
    try:
        m = float(max_val)
    except Exception:
        return minimum
    if m <= 0:
        return minimum
    return int(np.ceil(m / step) * step)

def format_pct2(pct_value: float) -> str:
    """
    å°‡ç™¾åˆ†æ¯”å››æ¨äº”å…¥åˆ°å°æ•¸é»å¾Œå…©ä½ï¼Œå›å‚³ 'xx.xx%'.
    ä½¿ç”¨ ROUND_HALF_UPï¼ˆä¸€èˆ¬å››æ¨äº”å…¥ï¼‰ã€‚
    """
    d = Decimal(str(pct_value)).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)
    # å¼·åˆ¶å…©ä½å°æ•¸æ ¼å¼
    return f"{d:.2f}%"

def draw_pie_with_side_legend(values, labels, colors, output_path,
                              title="",
                              startangle=90, edgecolor="#FFFFFF",
                              show_total=True, total_label="åˆè¨ˆ", dpi=150,
                              decimals=2):
    """
    å·¦é‚Šç•«åœ“é¤…ï¼›å³é‚Šç”¨è¡¨æ ¼é¡¯ç¤ºé¡åˆ¥èˆ‡ç™¾åˆ†æ¯”ï¼ˆå››æ¨äº”å…¥åˆ°å°æ•¸é»å¾Œå…©ä½ï¼‰ã€‚
    """
    vals = np.array(values, dtype=float)
    total = vals.sum()
    if total <= 0:
        plt.figure(figsize=(8,5), dpi=dpi)
        plt.text(0.5, 0.5, 'ç„¡è³‡æ–™', fontsize=18, ha='center', va='center')
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(output_path, bbox_inches='tight', dpi=dpi)
        plt.close()
        return

    fig = plt.figure(figsize=(8.5, 5.2), dpi=dpi)
    gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1], wspace=0.05)
    ax_pie = fig.add_subplot(gs[0, 0])
    ax_leg = fig.add_subplot(gs[0, 1])

    # å·¦ï¼šåœ“é¤…ï¼ˆæ‰‡å½¢ä¸Šä¸å°æ–‡å­—ï¼Œé¿å…é‡ç–Šï¼‰
    ax_pie.pie(vals, labels=None, colors=colors, startangle=startangle,
               wedgeprops=dict(edgecolor=edgecolor, linewidth=2))
    ax_pie.set_aspect('equal')
    if title:
        ax_pie.set_title(title, fontsize=18, pad=16)

    # å³ï¼šè¡¨æ ¼ + å…©ä½å°æ•¸ç™¾åˆ†æ¯”
    ax_leg.axis('off'); ax_leg.set_xlim(0,1); ax_leg.set_ylim(0,1)
    pct = vals / total * 100.0

    ax_leg.text(0.05, 0.95, "å€åŸŸ", fontsize=12, weight='bold', va='center')
    ax_leg.text(0.78, 0.95, "å æ¯”", fontsize=12, weight='bold', va='center')
    ax_leg.hlines(0.90, 0.03, 0.97, colors="#bdbdbd", linewidth=1)

    n = len(labels); top = 0.85; row_h = 0.12 if n <= 4 else 0.09
    for i, (lab, p, c) in enumerate(zip(labels, pct, colors)):
        y = top - i*row_h
        ax_leg.scatter(0.07, y, s=220, color=c)
        ax_leg.text(0.12, y, str(lab), fontsize=12, va='center')
        ax_leg.text(0.95, y, format_pct2(p), fontsize=12, va='center', ha='right')

    if show_total:
        ax_leg.hlines(top - n*row_h - 0.03, 0.03, 0.97, colors="#bdbdbd", linewidth=1)
        ax_leg.text(0.12, top - n*row_h - 0.10, total_label, fontsize=12, va='center')
        ax_leg.text(0.95, top - n*row_h - 0.10, "100.00%", fontsize=12, va='center', ha='right')

    fig.patch.set_facecolor("#fcfcfc")
    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight', dpi=dpi)
    plt.close()

# ====== åœ–è¡¨ï¼šå‹•æ…‹ Y è»¸å·¥å…·å‡½å¼ ======
def dynamic_ylim_from_values(vals):
    """
    ä»¥ 1-2-5-10 çš„ nice number è¦å‰‡è‡ªå‹•è¨ˆç®— Y è»¸ä¸Šé™ï¼Œç¢ºä¿å°é‡è³‡æ–™ä¸æœƒè¢«å£“æ‰ï¼Œ
    ä¹Ÿä¸æœƒç¡¬ç”Ÿç”Ÿè·³åˆ° 500ã€‚
    """
    m = int(max(vals)) if vals else 0
    if m <= 0:
        return 8  # æœ€ä½é«˜åº¦ï¼Œé¿å…åªæœ‰ 0 æ™‚åœ–å¤ªæ‰

    target = m * 1.15  # 15% headroom
    exp = math.floor(math.log10(target))
    base = target / (10 ** exp)

    if base <= 1:
        nice = 1
    elif base <= 2:
        nice = 2
    elif base <= 5:
        nice = 5
    else:
        nice = 10

    return int(nice * (10 ** exp))
# ====== åˆå§‹åŒ– colorama ======

init(autoreset=True)

# ====== ACL deny è¦†å¯«èˆ‡è¡Œç‚ºåˆ†ç´šï¼ˆå¼·åŒ–ï¼‰ ======
ATTEMPT_DENY_IDS = {"710003", "106023", "106021", "106016", "106015", "106014", "106006"}

def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    å°é½Š/è£œé½Šå¸¸ç”¨æ¬„ä½
    """
    df = df.copy()
    need_cols = [
        "event_id", "SyslogID", "Description", "raw_log", "Severity", "Datetime",
        "DestinationPort", "DestinationIP", "deny_cnt_60", "deny_u_dport_60",
        "deny_u_dip_60", "crlevel", "is_attack", "attack_type",
        "blocked", "src_if", "dst_if", "direction", "SourceIP"
    ]
    for c in need_cols:
        if c not in df.columns:
            df[c] = ""

    # ç”¨ SyslogID è£œ event_idï¼ˆåƒ…è£œç©º/NaNï¼‰
    eid_empty = df["event_id"].astype(str).str.strip().eq("") | df["event_id"].isna()
    df.loc[eid_empty, "event_id"] = df.loc[eid_empty, "SyslogID"].astype(str)

    # å‹åˆ¥
    df["is_attack"] = pd.to_numeric(df["is_attack"], errors="coerce").fillna(0).astype(int)
    df["blocked"] = pd.to_numeric(df["blocked"], errors="coerce").fillna(0).astype(int)
    return df

# ---------- ä»‹é¢/æ–¹å‘æŠ½å– ----------
def parse_interfaces_and_direction(df: pd.DataFrame) -> pd.DataFrame:
    """
    å¾ Description/raw_log æ¨æ–· dst_ifï¼ˆto outside:IP/port æˆ– on interface insideï¼‰
    ä¸¦æ¨ä¼° directionï¼ˆinside/outside -> dst_ifï¼‰
    """
    import re
    df = _normalize_columns(df)
    text = (df["Description"].astype(str) + " " + df["raw_log"].astype(str))

    # e.g. "to outside:120.125.85.60/443"
    m_to = text.str.extract(r"\bto\s+([A-Za-z0-9_-]+):\d+\.\d+\.\d+\.\d+/\d+", expand=True)
    # e.g. "on interface inside"
    m_on = text.str.extract(r"on\s+interface\s+([A-Za-z0-9_-]+)", expand=True)

    df.loc[m_to[0].notna(), "dst_if"] = m_to[0]
    df.loc[m_on[0].notna() & df["dst_if"].eq(""), "dst_if"] = m_on[0]

    def infer_dir(row):
        if row["dst_if"]:
            txt = f"{row['Description']} {row['raw_log']}".lower()
            # è‹¥æè¿°å…§å« outside æˆ–ä¾†æºç‚ºå…¬ç¶²ï¼Œè¦–ç‚º outside å‡ºç™¼
            try:
                ip = ipaddress.ip_address(str(row["SourceIP"]))
                is_private = ip.is_private
            except Exception:
                is_private = False
            src_if = "outside" if (("outside" in txt) or (not is_private)) else "inside"
            return f"{src_if}->{row['dst_if']}"
        return ""
    df["direction"] = df.apply(infer_dir, axis=1)
    return df

# ---------- èªæ„åµæ¸¬ ----------
def semantic_attack_labels(df: pd.DataFrame) -> pd.DataFrame:
    df = _normalize_columns(df)

    # event_id family
    eid = df["event_id"].astype(str).str.extract(r"(\d+)")[0].fillna("")
    mask_acl = eid.isin(ATTEMPT_DENY_IDS)

    low = (df["Description"].astype(str) + " " + df["raw_log"].astype(str)).str.lower()
    m_spoof = low.str.contains("ip spoof", na=False)
    m_no_conn = low.str.contains("no connection", na=False)
    m_deny = (
        low.str.contains(r"asa-\d+-710003", na=False) |
        low.str.contains("access denied", na=False)   |
        low.str.contains(r"\bdeny\b", na=False)       |
        low.str.contains(r"\bdenied\b", na=False)
    )

    mask_attack = mask_acl | m_deny | m_spoof | m_no_conn
    df.loc[mask_attack, "is_attack"] = 1
    df.loc[mask_attack, "blocked"] = 1

    df.loc[m_spoof, "attack_type"] = df["attack_type"].where(df["attack_type"].ne(""), "ip_spoof")
    df.loc[m_no_conn, "attack_type"] = df["attack_type"].where(df["attack_type"].ne(""), "out_of_state_tcp")
    df.loc[mask_acl | m_deny, "attack_type"] = df["attack_type"].where(df["attack_type"].ne(""), "deny_by_acl")
    return df

def add_acl_deny_window_stats(df: pd.DataFrame, window_sec: int = 60) -> pd.DataFrame:
    """
    æ ¹æ“š event_idï¼Œè¨ˆç®—æ¯ç­†åœ¨ window_sec ç§’å…§çš„ deny æ¬¡æ•¸ã€å”¯ä¸€ç›®çš„åŸ ã€å”¯ä¸€ç›®çš„IPã€‚
    å‡è¨­ 'Datetime' æ¬„ä½ç‚ºæ™‚é–“æˆ³å­—ä¸²ã€‚
    """
    import numpy as np

    df = _normalize_columns(df)
    if "Datetime" not in df.columns or "event_id" not in df.columns:
        df["deny_cnt_60"] = 0
        df["deny_u_dport_60"] = 0
        df["deny_u_dip_60"] = 0
        return df

    # åƒ…å°é—œå¿ƒçš„äº‹ä»¶åšçµ±è¨ˆ
    mask = df["event_id"].astype(str).isin(ATTEMPT_DENY_IDS)
    if not mask.any():
        df["deny_cnt_60"] = 0
        df["deny_u_dport_60"] = 0
        df["deny_u_dip_60"] = 0
        return df

    # è½‰æ™‚é–“ã€æ’åº
    d = df.copy()
    d["Datetime"] = pd.to_datetime(d["Datetime"], errors="coerce")
    d = d.sort_values("Datetime")

    d["deny_cnt_60"] = 0
    d["deny_u_dport_60"] = 0
    d["deny_u_dip_60"] = 0

    idxs = d[mask].index
    for idx in idxs:
        t = d.at[idx, "Datetime"]
        if pd.isna(t):
            continue
        eid = str(d.at[idx, "event_id"])
        window_start = t - pd.Timedelta(seconds=window_sec)
        win = d[
            (d["Datetime"] >= window_start) &
            (d["Datetime"] <= t) &
            (d["event_id"].astype(str) == eid)
        ]
        d.at[idx, "deny_cnt_60"] = len(win)
        d.at[idx, "deny_u_dport_60"] = win["DestinationPort"].nunique() if "DestinationPort" in win.columns else 0
        d.at[idx, "deny_u_dip_60"] = win["DestinationIP"].nunique() if "DestinationIP" in win.columns else 0

    cols = ["deny_cnt_60", "deny_u_dport_60", "deny_u_dip_60"]
    for c in cols:
        df[c] = d[c]
    return df

def overlay_acl_deny_rules(df: pd.DataFrame) -> pd.DataFrame:
    """å¿«é€Ÿè¦†å¯«ï¼šæ‰€æœ‰ ACL-deny æ—ç¾¤ â†’ ä¸€å¾‹æ¨™ç‚ºæ”»æ“Šå˜—è©¦ï¼ˆå·²é˜»æ“‹ï¼‰ã€‚"""
    df = _normalize_columns(df.copy())
    mask = df["event_id"].astype(str).str.extract(r"(\d+)")[0].isin(ATTEMPT_DENY_IDS)
    if "is_attack" not in df.columns:
        df["is_attack"] = 0
    if "attack_type" not in df.columns:
        df["attack_type"] = ""
    df.loc[mask, "is_attack"] = 1
    df.loc[mask, "attack_type"] = df["attack_type"].where(~mask, "deny_by_acl")
    df.loc[mask, "blocked"] = 1
    return df

def overlay_acl_deny_scoring(df: pd.DataFrame) -> pd.DataFrame:
    """ä¾ 60 ç§’å…§è¢«æ‹’æ¬¡æ•¸/å”¯ä¸€åŸ /å”¯ä¸€ç›®çš„IP æ±ºå®š crlevelï¼š1(é«˜), 2(ä¸­), 3(ä½)ã€‚"""
    import numpy as np

    df = _normalize_columns(df.copy())
    mask = df["event_id"].astype(str).isin(ATTEMPT_DENY_IDS)

    if "deny_cnt_60" not in df.columns:
        df.loc[mask, "crlevel"] = 2
        return df

    z = np.zeros(len(df))
    hi = mask & (
        (pd.to_numeric(df["deny_cnt_60"], errors="coerce").fillna(0) >= 10) |
        (pd.to_numeric(df.get("deny_u_dport_60", pd.Series(z)), errors="coerce").fillna(0) >= 5) |
        (pd.to_numeric(df.get("deny_u_dip_60",   pd.Series(z)), errors="coerce").fillna(0) >= 3)
    )
    med = mask & (~hi) & (
        (pd.to_numeric(df["deny_cnt_60"], errors="coerce").fillna(0).between(4, 9, inclusive="both")) |
        (pd.to_numeric(df.get("deny_u_dport_60", pd.Series(z)), errors="coerce").fillna(0).between(2, 4, inclusive="both"))
    )
    low = mask & (~hi) & (~med)

    df.loc[hi,  "crlevel"] = 1
    df.loc[med, "crlevel"] = 2
    df.loc[low, "crlevel"] = 3
    return df

def apply_acl_deny_overlay(df_pred: pd.DataFrame, window_sec: int = 60) -> pd.DataFrame:
    """åŠ è¦–çª—ç‰¹å¾µ â†’ ä¸€å¾‹æ¨™æ”»æ“Š â†’ è¡Œç‚ºåˆ†ç´šã€‚"""
    df_pred = add_acl_deny_window_stats(df_pred, window_sec=window_sec)
    df_pred = overlay_acl_deny_rules(df_pred)
    df_pred = overlay_acl_deny_scoring(df_pred)
    return df_pred

# ====== A+B è¦å‰‡è¨­å®š ======
DFLARE_A_STRICT_CODES = {"710003", "106023", "106021", "106016", "106015", "106014", "106006"}
DFLARE_A_TEXT_PATTERNS = [
    r"asa-\d+-710003",
    r"\baccess denied\b",
    r"\bdeny\b",
    r"\bdenied\b",
    r"\bno connection\b",
    r"\breverse path check\b"
]

DFLARE_B_CTX = {
    "window_sec": 60,
    "deny_cnt_hi": 6,
    "unique_dport_hi": 3,
    "unique_dip_hi": 2,
    "severity_gate": 4,
    "inside_if_markers": [" on interface inside", " on interface dmz"],
}

def _safe_append_tag(series: pd.Series, tag: str) -> pd.Series:
    """æŠŠ attack_type åŠ ä¸Šæ–°æ¨™ç±¤ï¼Œä¸é‡è¤‡ã€ä¸è¦†è“‹æ—¢æœ‰å…§å®¹ã€‚"""
    s = series.fillna("").astype(str)
    def _merge(v):
        if not v or v.strip() == "":
            return tag
        parts = [p for p in v.split("|") if p]
        if tag not in parts:
            parts.append(tag)
        return "|".join(parts)
    return s.apply(_merge)

def apply_acl_strict_and_context(df: pd.DataFrame, *, window_sec: int = None) -> pd.DataFrame:
    """
    Aï¼ˆåš´æ ¼ï¼‰ï¼šä»£ç¢¼/æ–‡å­—å‘½ä¸­ â†’ is_attack=1, tag=A_strict
    Bï¼ˆæƒ…å¢ƒï¼‰ï¼šè¦–çª—é–€æª»æˆ–æ–‡å­—ç·šç´¢/inside æç¤º â†’ is_attack=1, tag=B_context
    """
    df = _normalize_columns(df.copy())

    eid_num = df["event_id"].astype(str).str.extract(r"(\d+)")[0].fillna("")
    txt = (df.get("Description", "").astype(str) + " " + df.get("raw_log", "").astype(str)).str.lower()

    a_code = eid_num.isin(DFLARE_A_STRICT_CODES)
    import re
    a_text = pd.Series(False, index=df.index)
    for pat in DFLARE_A_TEXT_PATTERNS:
        a_text = a_text | txt.str.contains(pat, na=False, regex=True)

    a_mask = a_code | a_text
    if "is_attack" not in df.columns:
        df["is_attack"] = 0
    if "attack_type" not in df.columns:
        df["attack_type"] = ""

    df.loc[a_mask, "is_attack"] = 1
    df.loc[a_mask, "attack_type"] = _safe_append_tag(df.loc[a_mask, "attack_type"], "A_strict")

    _w = window_sec or DFLARE_B_CTX["window_sec"]
    df = add_acl_deny_window_stats(df, window_sec=_w)

    cnt = pd.to_numeric(df.get("deny_cnt_60", 0), errors="coerce").fillna(0)
    u_dport = pd.to_numeric(df.get("deny_u_dport_60", 0), errors="coerce").fillna(0)
    u_dip = pd.to_numeric(df.get("deny_u_dip_60", 0), errors="coerce").fillna(0)

    b_cnt = cnt >= DFLARE_B_CTX["deny_cnt_hi"]
    b_dport = u_dport >= DFLARE_B_CTX["unique_dport_hi"]
    b_dip = u_dip >= DFLARE_B_CTX["unique_dip_hi"]

    has_no_conn = txt.str.contains(r"\bno connection\b", na=False)
    has_rpf = txt.str.contains(r"\breverse path check\b", na=False)
    inside_hint = pd.Series(False, index=df.index)
    for m in DFLARE_B_CTX["inside_if_markers"]:
        inside_hint = inside_hint | txt.str.contains(re.escape(m), na=False)

    b_mask = b_cnt | b_dport | b_dip | has_no_conn | has_rpf | inside_hint
    b_only = b_mask & (~a_mask)

    df.loc[b_only, "is_attack"] = 1
    df.loc[b_only, "attack_type"] = _safe_append_tag(df.loc[b_only, "attack_type"], "B_context")
    return df

def force_mark_acl_deny_anywhere(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¥µé™ä¿éšªï¼šåµæ¸¬åˆ° 710003 æˆ– deny æ–‡å­—ï¼Œç„¡è«–å…¶ä»–çµæœï¼Œis_attack=1ã€‚
    """
    df = _normalize_columns(df.copy())

    sid = df.get("SyslogID", "").astype(str)
    eid = df.get("event_id", "").astype(str)

    sid_num = sid.str.extract(r"(\d+)")[0].fillna("")
    eid_num = eid.str.extract(r"(\d+)")[0].fillna("")
    m_id = sid_num.eq("710003") | eid_num.eq("710003")

    text = (df.get("Description", "").astype(str) + " " + df.get("raw_log", "").astype(str)).str.lower()
    m_txt = (
        text.str.contains(r"asa-\d+-710003", na=False) |
        text.str.contains("access denied", na=False)   |
        text.str.contains(r"\bdeny\b", na=False)       |
        text.str.contains(r"\bdenied\b", na=False)
    )

    mask = m_id | m_txt
    df.loc[mask, "is_attack"] = 1
    # è‹¥å·²æœ‰ attack_typeï¼Œä¸è¦†è“‹ï¼›å¦å‰‡æ¨™ä¸Š final tag
    df.loc[mask, "attack_type"] = df["attack_type"].where(~mask | df["attack_type"].astype(str).ne(""),
                                                          "deny_by_acl_final")
    df.loc[mask, "blocked"] = 1
    return df

# ---------- ç™½åå–® ----------
def load_whitelist(path: str):
    ip_set, net_list = set(), []
    if not path or not os.path.exists(path): return ip_set, net_list
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            s = line.strip()
            if not s or s.startswith("#"): continue
            try:
                if "/" in s: net_list.append(ipaddress.ip_network(s, strict=False))
                else: ip_set.add(ipaddress.ip_address(s))
            except Exception:
                continue
    return ip_set, net_list

def ip_in_whitelist(ip_str, ip_set, net_list) -> bool:
    try:
        ip = ipaddress.ip_address(str(ip_str))
    except Exception:
        return False
    if ip in ip_set: return True
    return any(ip in n for n in net_list)

def apply_whitelist(df: pd.DataFrame, ip_set, net_list) -> pd.DataFrame:
    df = _normalize_columns(df)
    wl = df["SourceIP"].apply(lambda x: ip_in_whitelist(x, ip_set, net_list)) | \
         df["DestinationIP"].apply(lambda x: ip_in_whitelist(x, ip_set, net_list))
    df.loc[wl, "is_attack"] = 0
    df.loc[wl, "attack_type"] = "whitelisted"
    df.loc[wl, "blocked"] = 0
    return df

def _redraw_binary_charts_from_df(df: pd.DataFrame, output_pie: str, output_bar: str):
    """æŠŠè¦†å¯«å¾Œçš„äºŒå…ƒçµæœé‡æ–°ç•«åœ–ï¼Œç¢ºä¿åœ–è¡¨èˆ‡ CSV ä¸€è‡´ã€‚"""
    import matplotlib.pyplot as plt
    from matplotlib.font_manager import FontProperties
    from matplotlib.ticker import MaxNLocator, FuncFormatter

    if os.name == "nt":
        font_path = "C:/Windows/Fonts/msjh.ttc"
    elif os.path.exists("/System/Library/Fonts/PingFang.ttc"):
        font_path = "/System/Library/Fonts/PingFang.ttc"
    else:
        font_path = "/usr/share/fonts/truetype/arphic/uming.ttc"
    font_name = FontProperties(fname=font_path).get_name()
    plt.rcParams['font.family'] = font_name
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.facecolor'] = "#fcfcfc"

    is_attack_dist = df['is_attack'].value_counts().sort_index().reindex([0, 1], fill_value=0)
    labels_attack = ['æ­£å¸¸æµé‡', 'æ”»æ“Šæµé‡']
    pie_base_colors = ["#04ff11", "#FF0000"]

    # åœ“é¤…
    values = [int(is_attack_dist.get(0, 0)), int(is_attack_dist.get(1, 0))]
    draw_pie_with_side_legend(
        values=values,
        labels=labels_attack,
        colors=pie_base_colors,
        output_path=output_pie,
        title="æ”»æ“Šèˆ‡æ­£å¸¸æµé‡æ¯”ä¾‹ï¼ˆäºŒå…ƒï¼‰",
        decimals=2
    )

    # é•·æ¢ï¼ˆå‹•æ…‹ Y è»¸ + åƒåˆ†ä½ + ç½®ä¸­æ•¸å­—ï¼‰
    plt.figure(figsize=(7, 5))
    ax = plt.gca()
    vals = [int(is_attack_dist.loc[0]), int(is_attack_dist.loc[1])]
    y_top = dynamic_ylim_from_values(vals)

    plt.bar(labels_attack, vals, color=pie_base_colors, edgecolor="#333", width=0.6)
    ax.set_ylim(0, y_top)
    ax.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
    ax.yaxis.grid(True, linewidth=1, alpha=0.25)
    for spine in ['top','right']:
        ax.spines[spine].set_visible(False)
    for idx, v in enumerate(vals):
        y_lab = min(v + y_top * 0.05, y_top * 0.98)
        ax.text(idx, y_lab, f"{int(v):,}", ha='center', va='bottom', fontsize=13)

    ax.set_xlabel('æµé‡é¡å‹', fontsize=15, labelpad=10)
    ax.set_ylabel('æ•¸é‡ï¼ˆç­†ï¼‰', fontsize=15, labelpad=10)
    ax.set_title('æ”»æ“Šèˆ‡æ­£å¸¸æµé‡æ•¸é‡åˆ†å¸ƒï¼ˆäºŒå…ƒï¼‰', fontsize=18, pad=18)
    plt.tight_layout()
    plt.savefig(output_bar, bbox_inches='tight')
    plt.close()

# ---------- å‘Šè­¦å½™æ•´ ----------
def build_alerts(df: pd.DataFrame, out_json: str):
    df = _normalize_columns(df)
    out = {}
    vc = df.loc[df["is_attack"]==1, "crlevel"].value_counts(dropna=False).to_dict()
    out["attack_crlevel_distribution"] = {str(k): int(v) for k,v in vc.items()}
    vt = df.loc[df["is_attack"]==1, "attack_type"].value_counts().to_dict()
    out["attack_type_distribution"] = {str(k): int(v) for k,v in vt.items()}
    vd = df.loc[df["is_attack"]==1, "direction"].value_counts().head(10).to_dict()
    out["top_directions"] = vd
    # æ¬„ä½å…¼å®¹
    src_col = "SourceIP" if "SourceIP" in df.columns else ("src_ip" if "src_ip" in df.columns else None)
    if src_col:
        vs = df.loc[df["is_attack"]==1, src_col].value_counts().head(10).to_dict()
        out["top_source_ip"] = {str(k): int(v) for k,v in vs.items()}
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

# ==== å–å¾—æ–°çš„ batch_id ====
def get_next_batch_id(all_results_path):
    if not os.path.exists(all_results_path):
        return 1
    try:
        df = pd.read_csv(all_results_path)
        if "batch_id" in df.columns and not df.empty:
            return int(df["batch_id"].max()) + 1
        else:
            return 1
    except Exception:
        return 1

# =============== STEP1 ===============
def step1_process_logs(raw_log_path, step1_out_path, unique_out_json, batch_id, show_progress=True):
    import gzip, io
    import chardet

    STANDARD_COLUMNS = [
        "batch_id", "Datetime", "SyslogID", "Severity", "SourceIP", "SourcePort",
        "DestinationIP", "DestinationPort", "Duration", "Bytes",
        "Protocol", "Action", "Description", "raw_log"
    ]
    unique_vals = {col: set() for col in STANDARD_COLUMNS}
    CHUNK_SIZE = 50000

    def detect_encoding_safely(file_path, sample_bytes=200_000):
        try:
            if str(file_path).lower().endswith(".gz"):
                with gzip.open(file_path, "rb") as f:
                    raw = f.read(sample_bytes)
            else:
                with open(file_path, "rb") as f:
                    raw = f.read(sample_bytes)
            enc = chardet.detect(raw).get("encoding") or "utf-8"
            return enc
        except Exception:
            return "utf-8"

    def count_lines_binary(file_path):
        bufsize = 1024 * 1024
        total = 0
        if str(file_path).lower().endswith(".gz"):
            with gzip.open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(bufsize), b""): total += chunk.count(b"\n")
        else:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(bufsize), b""): total += chunk.count(b"\n")
        return max(total, 1)

    def open_text_stream(file_path, encoding):
        if str(file_path).lower().endswith(".gz"):
            g = gzip.open(file_path, "rb")
            return io.TextIOWrapper(g, encoding=encoding, errors="replace", newline="")
        else:
            return open(file_path, "r", encoding=encoding, errors="replace", newline="")

    print(f"{Fore.CYAN}{Style.BRIGHT}STEP1ï¼šé–‹å§‹è³‡æ–™æ¸…æ´—èˆ‡å”¯ä¸€å€¼çµ±è¨ˆ")
    encoding = detect_encoding_safely(raw_log_path)
    total_lines = count_lines_binary(raw_log_path)

    processed_count = 0
    f = open_text_stream(raw_log_path, encoding)
    try:
        with f as tf, open(step1_out_path, "w", newline='', encoding="utf-8") as out_f:
            reader = csv.DictReader(tf)
            writer = csv.DictWriter(out_f, fieldnames=STANDARD_COLUMNS)
            writer.writeheader()
            pbar_total = max(total_lines - 1, 0)
            pbar = tqdm(reader, total=pbar_total, desc=f"{Fore.CYAN}æ¸…æ´—é€²åº¦", disable=not show_progress)

            for i, row in enumerate(pbar):
                if i < 2:
                    print("[STEP1] DEBUG row keys:", list(row.keys()))
                    print("[STEP1] DEBUG row content:", row)
                record = {col: row.get(col, "") for col in STANDARD_COLUMNS if col != "batch_id" and col != "raw_log"}
                record["batch_id"] = batch_id
                record["raw_log"] = json.dumps(row, ensure_ascii=False)

                for col in STANDARD_COLUMNS:
                    if col not in record:
                        record[col] = "unknown"
                    unique_vals[col].add(record[col])

                writer.writerow(record)
                processed_count += 1
                if (i + 1) % 10000 == 0:
                    pbar.set_postfix_str(
                        f"{Fore.YELLOW}å·²è™•ç† {i + 1} ç­†ï¼Œå”¯ä¸€å€¼ç´¯è¨ˆï¼š{ {k: len(v) for k, v in unique_vals.items()} }"
                    )
                if (i + 1) % CHUNK_SIZE == 0:
                    out_f.flush()
        print(f"{Fore.GREEN}{Style.BRIGHT}STEP1 çµæŸï¼Œå…±è™•ç† {processed_count} ç­†è³‡æ–™ã€‚")
    finally:
        try: f.close()
        except Exception: pass

    unique_json = {k: sorted([str(x) for x in v]) for k, v in unique_vals.items()}
    with open(unique_out_json, "w", encoding="utf-8") as fjson:
        json.dump(unique_json, fjson, ensure_ascii=False, indent=4)
    print(f"{Fore.GREEN}{Style.BRIGHT}STEP1 å®Œæˆï¼šå·²è¼¸å‡º {step1_out_path} åŠ {unique_out_json}")

# =============== STEP2 ===============
def step2_preprocess_data(step1_out_path, step2_out_path, unique_json, show_progress=True):
    with open(unique_json, "r", encoding="utf-8") as f:
        unique_vals = json.load(f)
    CATEGORICAL_MAPPINGS = {
        "Protocol": {
            "http": 1, "https": 2, "icmp": 3, "tcp": 4, "udp": 5,
            "scan": 6, "flood": 7, "other": 8, "unknown": 0, "nan": -1
        },
        "Action": {
            "built": 1, "teardown": 2, "deny": 3, "drop": 4,
            "login": 5, "other": 6, "unknown": 0, "nan": -1
        }
    }
    print(f"{Fore.CYAN}{Style.BRIGHT}STEP2ï¼šé–‹å§‹æ¬„ä½æ¨™æº–åŒ–èˆ‡æ˜ å°„")
    CHUNK_SIZE = 50000
    column_order = [
        "batch_id", "Datetime", "SyslogID", "Severity", "is_attack", "SourceIP", "SourcePort",
        "DestinationIP", "DestinationPort", "Duration", "Bytes", "Protocol",
        "Action", "Description", "raw_log"
    ]
    chunks = []
    total = sum(1 for _ in open(step1_out_path, encoding="utf-8"))
    processed = 0
    numeric_cols = ["SourcePort", "DestinationPort", "Duration", "Bytes"]

    # é€™è£¡ä¸ä»¥ Severity æ±ºå®š is_attackï¼ˆé¿å…è‰ç‡ï¼‰ï¼›å…ˆé è¨­ 0ï¼Œäº¤çµ¦å¾ŒçºŒè¦å‰‡/æ¨¡å‹
    for chunk in tqdm(pd.read_csv(step1_out_path, chunksize=CHUNK_SIZE), desc=f"{Fore.CYAN}é è™•ç†é€²åº¦", disable=not show_progress, total=total//CHUNK_SIZE+1):
        # é¡åˆ¥æ¬„ä½æ˜ å°„
        for col, mapping in CATEGORICAL_MAPPINGS.items():
            if col in chunk.columns:
                chunk[col] = chunk[col].astype(str).str.lower().map(mapping).fillna(-1).astype(int)

        # åˆåˆ¤ç›´æ¥ç¶­æŒ 0ï¼ˆæ¨¡å‹/è¦å‰‡æœƒå†è¦†å¯«ï¼‰
        chunk["is_attack"] = 0

        # æ•¸å€¼æ¬„ä½
        for col in numeric_cols:
            if col in chunk.columns:
                chunk[col] = pd.to_numeric(chunk[col], errors="coerce").fillna(0).astype(int)

        # æ¬„ä½é †åºèˆ‡è£œç¼º
        for col in column_order:
            if col not in chunk.columns:
                chunk[col] = ""
        chunk = chunk[column_order]
        chunks.append(chunk)
        processed += len(chunk)
        tqdm.write(f"{Fore.YELLOW}STEP2 ç´¯è¨ˆé è™•ç†ï¼š{processed} ç­†")

    df = pd.concat(chunks, ignore_index=True)
    before = len(df)
    df.drop_duplicates(inplace=True)
    after = len(df)
    print(f"{Fore.GREEN}å»é™¤é‡è¤‡è³‡æ–™ {before-after} ç­†ï¼Œå…±ä¿ç•™ {after} ç­†")
    df.to_csv(step2_out_path, index=False, encoding="utf-8")
    print(f"{Fore.GREEN}{Style.BRIGHT}STEP2 å®Œæˆï¼šå·²è¼¸å‡º {step2_out_path}")

# ========== STEP3-1ï¼šäºŒå…ƒæ¨¡å‹é æ¸¬+åœ–è¡¨ ==========
def dflare_binary_predict(input_csv, binary_model_path, output_csv, output_pie, output_bar, feat_cols=None, show_progress=True):
    import matplotlib.pyplot as plt
    from matplotlib.font_manager import FontProperties
    from matplotlib.ticker import MaxNLocator, FuncFormatter
    import joblib
    import os

    if os.name == "nt":
        font_path = "C:/Windows/Fonts/msjh.ttc"
    elif os.path.exists("/System/Library/Fonts/PingFang.ttc"):
        font_path = "/System/Library/Fonts/PingFang.ttc"
    else:
        font_path = "/usr/share/fonts/truetype/arphic/uming.ttc"
    font_name = FontProperties(fname=font_path).get_name()
    plt.rcParams['font.family'] = font_name
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.facecolor'] = "#fcfcfc"

    tqdm.write(f"{Fore.CYAN}{Style.BRIGHT}STEP3-1ï¼šè¼‰å…¥é è™•ç†è³‡æ–™èˆ‡äºŒå…ƒæ¨¡å‹...")
    df = pd.read_csv(input_csv, encoding="utf-8")
    bin_model = joblib.load(binary_model_path)

    # ç‰¹å¾µæ¬„ä½è‡ªå‹•å°é½Šï¼ˆåªå–æ¨¡å‹è¨“ç·´éçš„æ¬„ä½ï¼‰
    if hasattr(bin_model, "feature_names_in_"):
        model_feat_cols = list(bin_model.feature_names_in_)
    elif feat_cols is not None:
        model_feat_cols = feat_cols
    else:
        raise RuntimeError("äºŒå…ƒæ¨¡å‹æœªå­˜ç‰¹å¾µåç¨±ï¼Œè«‹æ˜ç¢ºæŒ‡å®š feat_colsã€‚")
    df_model = df.reindex(columns=model_feat_cols, fill_value=-1)
    df_model = df_model.fillna(-1).astype(int)

    tqdm.write(f"{Fore.CYAN}{Style.BRIGHT}STEP3-1ï¼šäºŒå…ƒæ¨¡å‹é æ¸¬èˆ‡åœ–è¡¨ç”¢ç”Ÿä¸­...")
    df['is_attack'] = bin_model.predict(df_model)
    df.to_csv(output_csv, index=False, encoding="utf-8")
    tqdm.write(f"{Fore.GREEN}{Style.BRIGHT}STEP3-1ï¼šäºŒå…ƒåˆ¤æ–·å·²è¼¸å‡º {output_csv}")

    # åœ–è¡¨
    is_attack_dist = df['is_attack'].value_counts().sort_index().reindex([0, 1], fill_value=0)
    values = [int(is_attack_dist.get(0, 0)), int(is_attack_dist.get(1, 0))]
    labels = ['æ­£å¸¸æµé‡', 'æ”»æ“Šæµé‡']
    colors = ["#04ff11", "#FF0000"]

    draw_pie_with_side_legend(
        values=values,
        labels=labels,
        colors=colors,
        output_path=output_pie,
        title="æ”»æ“Šèˆ‡æ­£å¸¸æµé‡æ¯”ä¾‹ï¼ˆäºŒå…ƒï¼‰",
        decimals=2
    )

    # é•·æ¢ï¼šå‹•æ…‹ Y è»¸ + åƒåˆ†ä½ + ç½®ä¸­æ•¸å­—
    plt.figure(figsize=(7, 5))
    ax = plt.gca()
    labels_attack = ['æ­£å¸¸æµé‡', 'æ”»æ“Šæµé‡']
    pie_base_colors = ["#04ff11", "#FF0000"]
    vals = [int(is_attack_dist.loc[0]), int(is_attack_dist.loc[1])]
    y_top = dynamic_ylim_from_values(vals)

    plt.bar(labels_attack, vals, color=pie_base_colors, edgecolor="#333", width=0.6)
    ax.set_ylim(0, y_top)
    ax.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
    ax.yaxis.grid(True, linewidth=1, alpha=0.25)
    for spine in ['top','right']:
        ax.spines[spine].set_visible(False)
    for idx, v in enumerate(vals):
        y_lab = min(v + y_top * 0.05, y_top * 0.98)
        ax.text(idx, y_lab, f"{int(v):,}", ha='center', va='bottom', fontsize=13)

    ax.set_xlabel('æµé‡é¡å‹', fontsize=15, labelpad=10)
    ax.set_ylabel('æ•¸é‡ï¼ˆç­†ï¼‰', fontsize=15, labelpad=10)
    ax.set_title('æ”»æ“Šèˆ‡æ­£å¸¸æµé‡æ•¸é‡åˆ†å¸ƒï¼ˆäºŒå…ƒï¼‰', fontsize=18, pad=18)
    plt.tight_layout()
    plt.savefig(output_bar, bbox_inches='tight')
    plt.close()

    return {
        "output_csv": output_csv,
        "output_pie": output_pie,
        "output_bar": output_bar,
        "is_attack_distribution": is_attack_dist.to_dict(),
        "count_all": int(df.shape[0]),
        "count_attack": int(is_attack_dist[1]),
        "count_normal": int(is_attack_dist[0]),
    }, df

# ========== STEP3-2ï¼šå¤šå…ƒæ¨¡å‹é æ¸¬+åœ–è¡¨ ==========
def dflare_multiclass_predict(df_attack, multiclass_model_path, output_csv, output_pie, output_bar, feat_cols=None, show_progress=True):
    import matplotlib.pyplot as plt
    from matplotlib.font_manager import FontProperties
    from matplotlib.ticker import MaxNLocator, FuncFormatter
    import joblib
    import os

    if os.name == "nt":
        font_path = "C:/Windows/Fonts/msjh.ttc"
    elif os.path.exists("/System/Library/Fonts/PingFang.ttc"):
        font_path = "/System/Library/Fonts/PingFang.ttc"
    else:
        font_path = "/usr/share/fonts/truetype/arphic/uming.ttc"
    font_name = FontProperties(fname=font_path).get_name()
    plt.rcParams['font.family'] = font_name
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.facecolor'] = "#fcfcfc"

    severity_map = {1: 'å±éšª', 2: 'é«˜', 3: 'ä¸­', 4: 'ä½'}
    show_levels = [1, 2, 3, 4]
    sev_labels = [severity_map[i] for i in show_levels]
    colors_sev = ["#d32f2f", "#f57c08", "#fbc02d", "#04ff11"]

    tqdm.write(f"{Fore.CYAN}{Style.BRIGHT}STEP3-2ï¼šè¼‰å…¥æ”»æ“Šæµé‡èˆ‡å¤šå…ƒæ¨¡å‹...")

    mul_model = joblib.load(multiclass_model_path)
    if hasattr(mul_model, "feature_names_in_"):
        model_feat_cols = list(mul_model.feature_names_in_)
    elif feat_cols is not None:
        model_feat_cols = feat_cols
    else:
        raise RuntimeError("å¤šå…ƒæ¨¡å‹æœªå­˜ç‰¹å¾µåç¨±ï¼Œè«‹æ˜ç¢ºæŒ‡å®š feat_colsã€‚")
    df_model = df_attack.reindex(columns=model_feat_cols, fill_value=-1)
    df_model = df_model.fillna(-1).astype(int)

    tqdm.write(f"{Fore.CYAN}{Style.BRIGHT}STEP3-2ï¼šå¤šå…ƒæ¨¡å‹åˆ†ç´šèˆ‡åœ–è¡¨ç”¢ç”Ÿä¸­...")
    df_attack['Severity'] = mul_model.predict(df_model)
    df_attack.to_csv(output_csv, index=False, encoding="utf-8")
    tqdm.write(f"{Fore.GREEN}{Style.BRIGHT}STEP3-2ï¼šå¤šå…ƒåˆ†ç´šå·²è¼¸å‡º {output_csv}")

    # åˆ†ä½ˆï¼ˆå¼·åˆ¶åŒ…å« 1..4ï¼‰
    sev_dist = df_attack['Severity'].value_counts().sort_index().reindex(show_levels, fill_value=0)
    vals = [int(sev_dist.loc[i]) for i in show_levels]

    # åœ“é¤…
    draw_pie_with_side_legend(
        values=vals,
        labels=sev_labels,
        colors=colors_sev,
        output_path=output_pie,
        title="Severity åˆ†å¸ƒï¼ˆåƒ…é‡å°æ”»æ“Šæµé‡ï¼‰",
        decimals=2
    )

    # é•·æ¢ï¼šå‹•æ…‹ Y è»¸ + åƒåˆ†ä½ + ç½®ä¸­æ•¸å­—
    plt.figure(figsize=(7, 5))
    ax = plt.gca(); ax.yaxis.set_major_locator(MaxNLocator(integer=True))
    y_top = dynamic_ylim_from_values(vals)
    ax.set_ylim(0, y_top)
    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
    plt.bar(sev_labels, vals, color=colors_sev, edgecolor="#333", width=0.6)
    for idx, v in enumerate(vals):
        y_lab = min(v + y_top * 0.05, y_top * 0.98)
        plt.text(idx, y_lab, f"{int(v):,}", ha='center', va='bottom', fontsize=13)
    plt.xlabel('Severity ç­‰ç´šï¼ˆ4ç‚ºæœ€ä½ï¼Œ1ç‚ºæœ€é«˜ï¼‰', fontsize=15, labelpad=10)
    plt.ylabel('æ•¸é‡ï¼ˆç­†ï¼‰', fontsize=15, labelpad=10)
    plt.title('Severity åˆ†å¸ƒï¼ˆåƒ…é‡å°æ”»æ“Šæµé‡ï¼‰', fontsize=18, pad=20)
    plt.tight_layout(); plt.savefig(output_bar, bbox_inches='tight'); plt.close()

    return {
        "output_csv": output_csv,
        "output_pie": output_pie,
        "output_bar": output_bar,
        "severity_distribution": sev_dist.to_dict(),
        "count_all": int(df_attack.shape[0]),
        "message": "å¤šå…ƒåˆ†ç´šçµæœå·²ç”¢ç”Ÿ"
    }

# ========== PIPELINE ==========
def dflare_sys_full_pipeline(
    raw_log_path, binary_model_path, multiclass_model_path, output_dir,
    all_results_csv="all_results.csv",
    bin_feat_cols=None, multi_feat_cols=None, show_progress=True,
    force_all_attack=False,
    overwrite_all_results=False,
    dedupe_all_results=False,
    whitelist_path=None
):
    os.makedirs(output_dir, exist_ok=True)

    # [1] æ‰¹æ¬¡æµæ°´è™Ÿ
    all_results_path = os.path.join(output_dir, all_results_csv)
    batch_id = get_next_batch_id(all_results_path)

    # [2] æš«å­˜æª”
    step1_out = os.path.join(output_dir, "processed_logs.csv")
    unique_json = os.path.join(output_dir, "log_unique_values.json")
    step2_out = os.path.join(output_dir, "preprocessed_data.csv")
    binary_csv = os.path.join(output_dir, "binary_result.csv")
    binary_pie = os.path.join(output_dir, "binary_pie.png")
    binary_bar = os.path.join(output_dir, "binary_bar.png")
    multi_csv = os.path.join(output_dir, "multiclass_result.csv")
    multi_pie = os.path.join(output_dir, "multiclass_pie.png")
    multi_bar = os.path.join(output_dir, "multiclass_bar.png")
    alerts_json = os.path.join(output_dir, "alerts.json")

    # [3] ETL
    step1_process_logs(raw_log_path, step1_out, unique_json, batch_id, show_progress)
    step2_preprocess_data(step1_out, step2_out, unique_json, show_progress)

    # [4] äºŒå…ƒæ¨¡å‹
    bin_res, df_bin = dflare_binary_predict(
        input_csv=step2_out,
        binary_model_path=binary_model_path,
        output_csv=binary_csv,
        output_pie=binary_pie,
        output_bar=binary_bar,
        feat_cols=bin_feat_cols,
        show_progress=show_progress
    )

    # === è¦†å¯«ï¼šèªæ„åµæ¸¬ â†’ è¦–çª—çµ±è¨ˆ â†’ è¡Œç‚ºåˆ†ç´š â†’ ä»‹é¢/æ–¹å‘ â†’ ç™½åå–® â†’ æ¥µé™ä¿éšª ===
    df_bin = apply_acl_deny_overlay(df_bin, window_sec=60)
    df_bin = apply_acl_strict_and_context(df_bin, window_sec=60)
    df_bin = add_acl_deny_window_stats(df_bin, window_sec=60)
    df_bin = overlay_acl_deny_scoring(df_bin)
    df_bin = parse_interfaces_and_direction(df_bin)
    df_bin = force_mark_acl_deny_anywhere(df_bin)
    df_bin["is_attack"] = pd.to_numeric(df_bin["is_attack"], errors="coerce").fillna(0).astype(int)

    ip_set, net_list = load_whitelist(whitelist_path) if whitelist_path else (set(), [])
    if ip_set or net_list:
        df_bin = apply_whitelist(df_bin, ip_set, net_list)

    df_bin = force_mark_acl_deny_anywhere(df_bin)  # æœ€å¾Œä¿éšª
    df_bin["is_attack"] = pd.to_numeric(df_bin["is_attack"], errors="coerce").fillna(0).astype(int)

    # å¯«å› & å‘Šè­¦å½™æ•´ï¼ˆåœ–è¡¨ç¨å¾Œæ ¹æ“šç´¯ç©è³‡æ–™é‡æ–°ç¹ªè£½ï¼‰
    df_bin.to_csv(binary_csv, index=False, encoding="utf-8")
    build_alerts(df_bin, alerts_json)

    # [5] å¤šå…ƒæ¨¡å‹ï¼ˆåƒ…æ”»æ“Šæµé‡ï¼‰
    print("=== PIPELINE DEBUG (is_attack åˆ†ä½ˆ) ===")
    print(df_bin["is_attack"].value_counts())
    df_attack = df_bin[df_bin['is_attack'] == 1].copy()
    if df_attack.empty:
        print(f"{Fore.YELLOW}{Style.BRIGHT}æœ¬æ‰¹è³‡æ–™ç„¡æ”»æ“Šæµé‡ï¼ˆis_attack=1ï¼‰ï¼Œè·³éå¤šå…ƒåˆ†ç´šã€‚")
        # [6] all_results å¯«å…¥
        df_bin["batch_id"] = batch_id
        if overwrite_all_results:
            df_bin.to_csv(all_results_path, index=False, encoding="utf-8")
        elif dedupe_all_results and os.path.exists(all_results_path):
            prev = pd.read_csv(all_results_path, encoding="utf-8")
            merged = pd.concat([prev, df_bin], ignore_index=True)
            if "raw_log" in merged.columns:
                merged = merged.drop_duplicates(subset=["raw_log"], keep="last")
            else:
                dedupe_keys = ["Datetime","SyslogID","SourceIP","SourcePort","DestinationIP","DestinationPort","Description"]
                dedupe_keys = [k for k in dedupe_keys if k in merged.columns]
                merged = merged.drop_duplicates(subset=dedupe_keys, keep="last")
            merged.to_csv(all_results_path, index=False, encoding="utf-8")
        else:
            write_header = not os.path.exists(all_results_path)
            with open(all_results_path, "a", newline='', encoding="utf-8") as allf:
                df_bin.to_csv(allf, header=write_header, index=False)
        # é‡æ–°è®€å–ç´¯ç©è³‡æ–™ä¸¦æ›´æ–°åœ–è¡¨
        df_all = pd.read_csv(all_results_path, encoding="utf-8")
        _redraw_binary_charts_from_df(df_all, binary_pie, binary_bar)
        dist_all = df_all['is_attack'].value_counts().sort_index().reindex([0,1], fill_value=0)
        bin_res["is_attack_distribution"] = dist_all.to_dict()
        bin_res["count_all"] = int(df_all.shape[0])
        bin_res["count_attack"] = int(dist_all.get(1, 0))
        bin_res["count_normal"] = int(dist_all.get(0, 0))
        return {
            "batch_id": batch_id,
            "binary": bin_res,
            "multiclass": None,
            "binary_output_csv": binary_csv,
            "binary_output_pie": binary_pie,
            "binary_output_bar": binary_bar,
            "alerts_json": alerts_json,
            "multiclass_output_csv": None,
            "multiclass_output_pie": None,
            "multiclass_output_bar": None,
            "all_results_csv": all_results_path,
            "message": "æœ¬æ‰¹ç„¡æ”»æ“Šæµé‡ï¼Œå·²è·³éå¤šå…ƒåˆ†ç´š"
        }

    # æœ‰æ”»æ“Šæµé‡æ‰é€²è¡Œå¤šå…ƒåˆ†ç´š
    multi_res = dflare_multiclass_predict(
        df_attack=df_attack,
        multiclass_model_path=multiclass_model_path,
        output_csv=multi_csv,
        output_pie=multi_pie,
        output_bar=multi_bar,
        feat_cols=multi_feat_cols,
        show_progress=show_progress
    )

    # [6] all_results å¯«å…¥
    df_bin["batch_id"] = batch_id
    if overwrite_all_results:
        df_bin.to_csv(all_results_path, index=False, encoding="utf-8")
    elif dedupe_all_results and os.path.exists(all_results_path):
        prev = pd.read_csv(all_results_path, encoding="utf-8")
        merged = pd.concat([prev, df_bin], ignore_index=True)
        if "raw_log" in merged.columns:
            merged = merged.drop_duplicates(subset=["raw_log"], keep="last")
        else:
            dedupe_keys = ["Datetime","SyslogID","SourceIP","SourcePort","DestinationIP","DestinationPort","Description"]
            dedupe_keys = [k for k in dedupe_keys if k in merged.columns]
            merged = merged.drop_duplicates(subset=dedupe_keys, keep="last")
        merged.to_csv(all_results_path, index=False, encoding="utf-8")
    else:
        write_header = not os.path.exists(all_results_path)
        with open(all_results_path, "a", newline='', encoding="utf-8") as allf:
            df_bin.to_csv(allf, header=write_header, index=False)

    # é‡æ–°è®€å–ç´¯ç©è³‡æ–™ä¸¦æ›´æ–°åœ–è¡¨
    df_all = pd.read_csv(all_results_path, encoding="utf-8")
    _redraw_binary_charts_from_df(df_all, binary_pie, binary_bar)
    dist_all = df_all['is_attack'].value_counts().sort_index().reindex([0,1], fill_value=0)
    bin_res["is_attack_distribution"] = dist_all.to_dict()
    bin_res["count_all"] = int(df_all.shape[0])
    bin_res["count_attack"] = int(dist_all.get(1, 0))
    bin_res["count_normal"] = int(dist_all.get(0, 0))

    # [7] å›å‚³
    return {
        "batch_id": batch_id,
        "binary": bin_res,
        "multiclass": multi_res,
        "binary_output_csv": binary_csv,
        "binary_output_pie": binary_pie,
        "binary_output_bar": binary_bar,
        "alerts_json": alerts_json,
        "multiclass_output_csv": multi_csv,
        "multiclass_output_pie": multi_pie,
        "multiclass_output_bar": multi_bar,
        "all_results_csv": all_results_path,
    }

# ==================== CLI/GUI å…±ç”¨ä¸»å…¥å£ ====================
if __name__ == "__main__":
    import argparse
    import tkinter as tk
    from tkinter import filedialog
    from tkinter import messagebox
    import os

    parser = argparse.ArgumentParser(
        description="D-FLARE SYS å…¨æµç¨‹è‡ªå‹•æ‰¹æ¬¡å·¥å…·ï¼ˆCisco ASA 5506-Xï¼‰"
    )
    parser.add_argument('--raw_log', type=str, help='åŸå§‹ log æª”è·¯å¾‘ï¼ˆcsv/txt/gzï¼‰')
    parser.add_argument('--bin_model', type=str, help='äºŒå…ƒæ¨¡å‹ .pkl è·¯å¾‘')
    parser.add_argument('--multi_model', type=str, help='å¤šå…ƒæ¨¡å‹ .pkl è·¯å¾‘')
    parser.add_argument('--output_dir', type=str, help='æ‰€æœ‰è¼¸å‡ºæª”å­˜æ”¾è³‡æ–™å¤¾')
    parser.add_argument('--no_progress', action="store_true", help='ä¸é¡¯ç¤ºé€²åº¦æ¢')
    parser.add_argument('--force_all_attack', action="store_true', help='æœ¬æ‰¹è³‡æ–™å…¨éƒ¨è¦–ç‚ºæ”»æ“Šï¼ˆè·³éäºŒå…ƒèª¤åˆ¤ï¼‰")
    parser.add_argument('--overwrite_all_results', action="store_true",
                        help='ä¸ç´¯ç©æ­·å²ï¼Œall_results.csv ç›´æ¥è¦†è“‹ç‚ºæœ¬æ‰¹çµæœ')
    parser.add_argument('--dedupe_all_results', action="store_true",
                        help='ç´¯ç©æ­·å²ï¼Œä½†ä»¥ raw_logï¼ˆæˆ–é—œéµæ¬„ä½ï¼‰å»é‡ï¼Œä¿ç•™è¼ƒæ–°æ‰¹æ¬¡')
    parser.add_argument('--whitelist', type=str, help='ç™½åå–®æª”æ¡ˆï¼ˆæ¯è¡Œ IP æˆ– CIDRï¼‰')
    args = parser.parse_args()

    # ç”¨ tkinter é¸æª” (äº’å‹•)
    def pick_file(title="è«‹é¸æ“‡æª”æ¡ˆ", filetypes=[("æ‰€æœ‰æª”æ¡ˆ", "*.*")]):
        root = tk.Tk(); root.withdraw()
        file_path = filedialog.askopenfilename(title=title, filetypes=filetypes)
        root.destroy(); return file_path

    def pick_folder(title="è«‹é¸æ“‡è³‡æ–™å¤¾"):
        root = tk.Tk(); root.withdraw()
        folder_path = filedialog.askdirectory(title=title)
        root.destroy(); return folder_path

    # æª¢æŸ¥åƒæ•¸ï¼Œè‹¥æ²’æŒ‡å®šå°±äº’å‹•é¸æ“‡
    raw_log = args.raw_log or pick_file("è«‹é¸æ“‡åŸå§‹ log æª”", [("CSV", "*.csv"), ("æ–‡å­—", "*.txt"), ("GZ", "*.gz"), ("æ‰€æœ‰æª”æ¡ˆ", "*.*")])
    bin_model = args.bin_model or pick_file("è«‹é¸æ“‡äºŒå…ƒæ¨¡å‹ .pkl æª”", [("Pickle", "*.pkl")])
    multi_model = args.multi_model or pick_file("è«‹é¸æ“‡å¤šå…ƒæ¨¡å‹ .pkl æª”", [("Pickle", "*.pkl")])
    output_dir = args.output_dir or pick_folder("è«‹é¸æ“‡è¼¸å‡ºè³‡æ–™å¤¾")
    show_progress = not args.no_progress

    print(f"{Fore.CYAN}{Style.BRIGHT}ğŸš¦ å•Ÿå‹• D-FLARE SYS å…¨æµç¨‹ Pipelineï¼ˆäºŒå…ƒ+å¤šå…ƒï¼Œå«èªæ„è¦†å¯«/è¡Œç‚ºåˆ†ç´š/ç™½åå–®/å‘Šè­¦å½™æ•´ï¼‰...")
    t0 = time.time()
    result = dflare_sys_full_pipeline(
        raw_log_path=raw_log,
        binary_model_path=bin_model,
        multiclass_model_path=multi_model,
        output_dir=output_dir,
        bin_feat_cols=None,
        multi_feat_cols=None,
        show_progress=show_progress,
        force_all_attack=args.force_all_attack,
        overwrite_all_results=args.overwrite_all_results,
        dedupe_all_results=args.dedupe_all_results,
        whitelist_path=args.whitelist
    )
    print(f"{Fore.GREEN}{Style.BRIGHT}ğŸš€ å…¨æµç¨‹å®Œæˆï¼çµæœå·²è¼¸å‡ºè‡³ï¼š{output_dir}")
    print(json.dumps(result, ensure_ascii=False, indent=2))
    print(f"{Fore.CYAN}{Style.BRIGHT}ç¸½è€—æ™‚ï¼š{time.time() - t0:.1f} ç§’ã€‚")
