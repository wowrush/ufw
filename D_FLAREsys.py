# dflare_sys.pyï¼ˆéé˜»å¡ç©©å®šç‰ˆï½œç´¯åŠ CSVï¼‹é˜²å¡é “ï¼‰
"""
D-FLARE SYS â€” å¤šå±¤é›†æˆå¼é˜²ç«ç‰†å¨è„…åˆ†ç´šç³»çµ±ï¼ˆCisco ASA 5506-X å°ˆç”¨ï¼‰
- é‡è¦æ”¹å‹•ï¼ˆé˜²å¡é “ï¼‰ï¼š
  1) Matplotlib ä½¿ç”¨ Agg å¾Œç«¯ï¼Œå®Œå…¨ä¸ä½”ç”¨ GUI è³‡æºã€‚
  2) Discord æ¨æ’­åƒ…ç”¨å½™æ•´å»é‡ï¼Œæä¾›æœ€å°æ›´æ–°é–“éš”ï¼Œé¿å…åˆ·å±èˆ‡é˜»å¡ã€‚
  3) step1 å”¯ä¸€å€¼çµ±è¨ˆåŠ ä¸Šä¸Šé™ï¼Œé¿å…è¶…å¤§è³‡æ–™é›†åƒçˆ†è¨˜æ†¶é«”ã€‚
  4) ç´¯ç©æª”ï¼ˆall_results.csv / all_multiclass_results.csvï¼‰æ”¹ç‚º **ç›´æ¥é™„åŠ **ï¼Œ
     åœ–è¡¨åˆ†ä½ˆç”¨ **chunk è®€å–çµ±è¨ˆ**ï¼Œä¸å†æŠŠæ•´æª”è¼‰å…¥è¨˜æ†¶é«”ï¼Œé¿å…å¤§æª”é€ æˆå¡é “ã€‚
  5) ç§»é™¤ tkinter äº’å‹•é¸æª”ï¼ˆé¿å…å¡ä½ä¸»æµç¨‹ï¼‰ï¼Œçµ±ä¸€æ”¹ç”¨ CLI åƒæ•¸ã€‚

- åœ–è¡¨ä»æœƒè¼¸å‡º PNGï¼Œä½†ç¹ªåœ–æœ¬èº«ä¸æœƒå½±éŸ¿ UIï¼ˆAgg + é—œé–‰åœ–çª—ï¼‰ã€‚
- è‹¥è¦é€²ä¸€æ­¥å°‡æ•´å€‹æµç¨‹èˆ‡ UI è§£è€¦ï¼Œè«‹æ­é…æˆ‘æä¾›çš„ PyQt éé˜»å¡éª¨æ¶æ•´åˆã€‚
"""

import os
import sys
import csv
import json
import time
import math
import ipaddress
import pandas as pd
from tqdm import tqdm
from colorama import init, Fore, Style

# ===== é˜²å¡é “ï¼šä½¿ç”¨é GUI å¾Œç«¯ =====
import matplotlib
matplotlib.use("Agg")  # ä¸ä½¿ç”¨ä»»ä½•äº’å‹•å¼å¾Œç«¯ï¼Œé¿å…é˜»å¡ UI

# ç¹ªåœ–
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

# ç™¾åˆ†æ¯”å››æ¨äº”å…¥ï¼ˆå…©ä½å°æ•¸ï¼‰
from decimal import Decimal, ROUND_HALF_UP

import requests
import hashlib
import re

# åˆå§‹åŒ– colorama
init(autoreset=True)

# ==================== åŸºç¤å·¥å…· ====================

def _coerce_severity(series: pd.Series) -> pd.Series:
    """æŠŠ Severity å®‰å…¨è½‰ç‚º 1~4 çš„æ•´æ•¸ï¼Œç„¡æ³•è§£æè¨­ç‚º NaNï¼Œæ–¹ä¾¿å¾ŒçºŒ reindexã€‚"""
    s = pd.to_numeric(series, errors="coerce")
    return s

_FLOOD_PAT = re.compile(r"(?:\bflood\b|syn\s*flood|rst\s*flood|embryonic)", re.I)

def _is_flood_text(s: str) -> bool:
    if not s:
        return False
    return bool(_FLOOD_PAT.search(s))

def _make_struct_signature(rows: pd.DataFrame) -> tuple[str, bool]:
    """
    ä¾ã€Œæ˜¯å¦å«æ´ªæ°´ã€+ ã€ŒSeverity-SyslogID çµ„åˆã€ç”¢ç”Ÿçµæ§‹ç°½ç« ï¼ˆèˆ‡æ•¸é‡ç„¡é—œï¼‰ã€‚
    ç”¨æ–¼å»é‡ï¼Œé¿å…ç›¸åŒé¡å‹åœ¨çŸ­æ™‚é–“å…§é‡è¤‡æ¨æ’­ã€‚
    """
    has_flood = bool(rows.get("is_flood", pd.Series(dtype=bool))).any()
    keys = set()
    for _, r in rows.iterrows():
        sev = int(r["Severity"]) if pd.notna(r["Severity"]) else 0
        sid = str(r["SyslogID"]) if pd.notna(r["SyslogID"]) else "?"
        keys.add(f"{sev}-{sid}")
    base = ("FLOOD|" if has_flood else "GEN|") + "|".join(sorted(keys))
    return hashlib.sha1(base.encode("utf-8", errors="ignore")).hexdigest(), has_flood

def _agg_for_discord(df_attack: pd.DataFrame) -> tuple[str, bool, int, list[str]]:
    """
    å°‡æ”»æ“Šæµé‡å½™æ•´æˆã€Œsev+syslog_idã€æ¸…å–®èˆ‡ç°½ç« ã€‚
    é è¨­åŒ…å« Severity 1~4ï¼Œè‹¥è¦åªå« 1~3 å¯è‡ªè¡Œèª¿æ•´ã€‚
    """
    if df_attack.empty:
        return "", False, 0, []

    tmp = df_attack.copy()
    # åˆ¤æ–·æ´ªæ°´
    text = (tmp.get("Description", "").astype(str) + " " + tmp.get("raw_log", "").astype(str))
    tmp["is_flood"] = text.apply(_is_flood_text)

    # å– sev 1~4
    tmp["Severity"] = _coerce_severity(tmp.get("Severity", pd.Series(dtype=float)))
    tmp = tmp[tmp["Severity"].isin([1, 2, 3, 4])]

    if tmp.empty:
        return "", False, 0, []

    grp = tmp.groupby(["Severity", "SyslogID", "is_flood"], dropna=False).size().reset_index(name="count")

    sig, has_flood = _make_struct_signature(grp)
    total = int(grp["count"].sum())
    lines = []
    # æ’åºï¼šSeverity ç”±é«˜åˆ°ä½(1â†’4)ï¼ŒåŒå±¤ä¾æ•¸é‡å¤šâ†’å°‘
    grp = grp.sort_values(by=["Severity", "count"], ascending=[True, False])
    for _, r in grp.iterrows():
        sev = int(r["Severity"])  # 1 é«˜ â†’ 4 ä½
        sid = str(r["SyslogID"]) if pd.notna(r["SyslogID"]) else "?"
        cnt = int(r["count"])
        suf = " (FLOOD)" if bool(r["is_flood"]) else ""
        lines.append(f"sev{sev} id{sid} x{cnt}{suf}")

    return sig, has_flood, total, lines

# ---- è¼•é‡ç‹€æ…‹æª” I/O ----

def _load_state(path: str) -> dict:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_state(path: str, data: dict):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

# ---- éé˜»å¡ï¼šç¸®çŸ­ timeoutï¼Œé¿å…ç­‰å¾…å¡æ­» ----

def _discord_post(url: str, content: str) -> str | None:
    try:
        r = requests.post(url.rstrip("/") + "?wait=true", json={"content": content}, timeout=3)
        if r.status_code in (200, 204):
            try:
                data = r.json()
                return str(data.get("id")) if isinstance(data, dict) else None
            except Exception:
                return None
    except Exception:
        return None

def _discord_patch(url: str, message_id: str, content: str) -> bool:
    try:
        r = requests.patch(url.rstrip("/") + f"/messages/{message_id}", json={"content": content}, timeout=3)
        return r.status_code in (200, 204)
    except Exception:
        return False

# ---- å½™æ•´+å»é‡+æœ€å°é–“éš”ï¼Œé¿å…åˆ·å±å’Œé »ç¹ç¶²è·¯å¡é “ ----

def notify_discord_dedup(
    df_attack: pd.DataFrame,
    webhook_url: str,
    state_path: str,
    batch_id: int,
    dedup_sec: int = 120,
    growth_ratio: float = 0.5,
    update_instead: bool = True,
    *,
    only_once: bool = True,           # â˜… æ–°å¢ï¼šé è¨­ one-shotï¼Œå¾ŒçºŒä¸€å¾‹è·³é
    only_once_update: bool = False,   # â˜… æˆ–ç”¨é€™å€‹ï¼šå¾ŒçºŒåª PATCHï¼Œä¸å† POST æ–°è¨Šæ¯
) -> dict:
    """
    - one-shot æ¨¡å¼ï¼š
      only_once=True            â†’ åŒç°½ç« åªç™¼ä¸€æ¬¡ï¼Œå…¶å¾Œæ°¸é  skipã€‚
      only_once_update=True     â†’ åŒç°½ç« åªç™¼ä¸€æ¬¡ï¼Œå…¶å¾Œåƒ… PATCH æ›´æ–°ï¼›è‹¥ PATCH å¤±æ•—ä¹Ÿè·³éï¼ˆä¸æ–°é–‹ï¼‰ã€‚

    - è‹¥å…©è€…çš†ç‚º Falseï¼Œå‰‡ä½¿ç”¨åŸæœ¬ã€Œè¦–çª—+æˆé•·æ¯”ä¾‹ã€ç­–ç•¥ï¼Œä½†åšä¸€å€‹é—œéµæ”¹å‹•ï¼š
      ç•¶ PATCH å¤±æ•—æ™‚ã€ä¸å† fallback ç‚º POSTã€ï¼Œé¿å…åˆ·å±ã€‚
    """
    assert not (only_once and only_once_update), "only_once èˆ‡ only_once_update è«‹æ“‡ä¸€"

    sig, has_flood, total, lines = _agg_for_discord(df_attack)
    if not lines:
        return {"action": "skip", "message_id": None, "reason": "no_lines"}

    title = "ğŸš¨ æ´ªæ°´/ç•°å¸¸é«˜æµé‡æ‘˜è¦" if has_flood else f"äº‹ä»¶æ‘˜è¦ï¼ˆæ‰¹æ¬¡ {batch_id}ï¼‰"
    content = title + "\n" + "\n".join(lines[:25])

    state = _load_state(state_path)
    now = time.time()
    prev = state.get(sig)

    # ---------- one-shotï¼ˆåªç™¼ä¸€æ¬¡ï¼‰ ----------
    if prev is not None:
        # å·²ç¶“ç™¼é
        if only_once:
            return {"action": "skip", "message_id": prev.get("message_id"), "reason": "only_once"}
        if only_once_update:
            mid = prev.get("message_id")
            if mid:
                # åªå˜—è©¦ PATCHï¼Œä¸æˆåŠŸä¹Ÿä¸é–‹æ–°è¨Šæ¯ï¼ˆé¿å…é‡è¤‡ï¼‰
                ok = _discord_patch(webhook_url, mid, content)
                if ok:
                    prev.update({"ts": now, "count": total})
                    state[sig] = prev
                    _save_state(state_path, state)
                    return {"action": "update", "message_id": mid}
                else:
                    return {"action": "skip", "message_id": mid, "reason": "patch_failed_skip"}
            else:
                # ç†è«–ä¸Šä¸æœƒç™¼ç”Ÿï¼Œä½†ä¿éšªè™•ç†ï¼šè£œç™¼ä¸€æ¬¡å¾Œå°±å›ºå®šç”¨é€™å‰‡
                mid = _discord_post(webhook_url, content)
                state[sig] = {"ts": now, "count": total, "message_id": mid}
                _save_state(state_path, state)
                return {"action": "send", "message_id": mid}

    # é‚„æ²’ç™¼é
    if only_once or only_once_update:
        mid = _discord_post(webhook_url, content)
        state[sig] = {"ts": now, "count": total, "message_id": mid}
        _save_state(state_path, state)
        return {"action": "send", "message_id": mid}

    # ---------- å‚³çµ±å»é‡ï¼ˆæ™‚é–“çª— + æˆé•·æ¯”ä¾‹ï¼‰ï¼Œä½†ç¦æ­¢ PATCH å¤±æ•—å¾Œ fallback ç‚º POST ----------
    if (prev is None) or (now - float(prev.get("ts", 0)) > dedup_sec):
        mid = _discord_post(webhook_url, content)
        state[sig] = {"ts": now, "count": total, "message_id": mid}
        _save_state(state_path, state)
        return {"action": "send", "message_id": mid}

    # è¦–çª—å…§ â†’ æª¢æŸ¥æˆé•·æ¯”ä¾‹
    prev_cnt = max(int(prev.get("count", 1)), 1)
    growth = (total - prev_cnt) / prev_cnt
    if growth >= growth_ratio:
        if update_instead and prev.get("message_id"):
            ok = _discord_patch(webhook_url, prev["message_id"], content)
            if ok:
                prev.update({"ts": now, "count": total})
                state[sig] = prev
                _save_state(state_path, state)
                return {"action": "update", "message_id": prev["message_id"]}
            # é—œéµæ”¹å‹•ï¼šPATCH å¤±æ•— â†’ ç›´æ¥ skipï¼ˆä¸å† POST æ–°è¨Šæ¯ä»¥é¿å…åˆ·å±ï¼‰
            return {"action": "skip", "message_id": prev.get("message_id"), "reason": "patch_failed_skip"}

    return {"action": "skip", "message_id": prev.get("message_id"), "reason": "growth_below_threshold"}


    title = "ğŸš¨ æ´ªæ°´/ç•°å¸¸é«˜æµé‡æ‘˜è¦" if has_flood else f"äº‹ä»¶æ‘˜è¦ï¼ˆæ‰¹æ¬¡ {batch_id}ï¼‰"
    content = title + "\n" + "\n".join(lines[:25])

    state = _load_state(state_path)
    now = time.time()
    prev = state.get(sig)

    # æ–°äº‹ä»¶æˆ–éæœŸ â†’ ç™¼æ–°è¨Šæ¯
    if (prev is None) or (now - float(prev.get("ts", 0)) > dedup_sec):
        mid = _discord_post(webhook_url, content)
        state[sig] = {"ts": now, "count": total, "message_id": mid, "last_patch": now}
        _save_state(state_path, state)
        return {"action": "send", "message_id": mid}

    # è¦–çª—å…§ â†’ æª¢æŸ¥æˆé•·æ¯”ä¾‹ + æœ€å°æ›´æ–°é–“éš”
    prev_cnt = max(int(prev.get("count", 1)), 1)
    growth = (total - prev_cnt) / prev_cnt
    last_patch = float(prev.get("last_patch", prev.get("ts", 0)))
    if (growth >= growth_ratio) and (now - last_patch >= min_update_interval_sec):
        if update_instead and prev.get("message_id"):
            ok = _discord_patch(webhook_url, prev["message_id"], content)
            if ok:
                prev.update({"ts": now, "count": total, "last_patch": now})
                state[sig] = prev
                _save_state(state_path, state)
                return {"action": "update", "message_id": prev["message_id"]}
        # PATCH å¤±æ•—æˆ–ä¸æ›´æ–° â†’ ç™¼æ–°è¨Šæ¯
        mid = _discord_post(webhook_url, content)
        state[sig] = {"ts": now, "count": total, "message_id": mid, "last_patch": now}
        _save_state(state_path, state)
        return {"action": "send", "message_id": mid}

    # æˆé•·ä¸è¶³æˆ–é–“éš”æœªåˆ° â†’ skipï¼ˆä¸è§¸ç¶²è·¯ï¼‰
    prev["ts"] = now
    state[sig] = prev
    _save_state(state_path, state)
    return {"action": "skip", "message_id": prev.get("message_id")}


def ceil_to_step(max_val, step=500, minimum=500):
    try:
        m = float(max_val)
    except Exception:
        return minimum
    if m <= 0:
        return minimum
    return int(np.ceil(m / step) * step)

def format_pct2(pct_value: float) -> str:
    d = Decimal(str(pct_value)).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)
    return f"{d:.2f}%"

# ==================== åœ–è¡¨å·¥å…· ====================

def draw_pie_with_side_legend(values, labels, colors, output_path,
                              title="", startangle=90, edgecolor="#FFFFFF",
                              show_total=True, total_label="åˆè¨ˆ", dpi=150,
                              decimals=2):
    vals = np.array(values, dtype=float)
    total = vals.sum()
    if total <= 0:
        plt.figure(figsize=(8, 5), dpi=dpi)
        plt.text(0.5, 0.5, 'ç„¡è³‡æ–™', fontsize=18, ha='center', va='center')
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(output_path, bbox_inches='tight', dpi=dpi)
        plt.close()
        return

    fig = plt.figure(figsize=(8.5, 5.2), dpi=dpi)
    gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1], wspace=0.05)
    ax_pie = fig.add_subplot(gs[0, 0])
    ax_leg = fig.add_subplot(gs[0, 1])

    ax_pie.pie(vals, labels=None, colors=colors, startangle=startangle,
               wedgeprops=dict(edgecolor=edgecolor, linewidth=2))
    ax_pie.set_aspect('equal')
    if title:
        ax_pie.set_title(title, fontsize=18, pad=16)

    ax_leg.axis('off'); ax_leg.set_xlim(0, 1); ax_leg.set_ylim(0, 1)
    pct = vals / total * 100.0
    ax_leg.text(0.05, 0.95, "å€åŸŸ", fontsize=12, weight='bold', va='center')
    ax_leg.text(0.78, 0.95, "å æ¯”", fontsize=12, weight='bold', va='center')
    ax_leg.hlines(0.90, 0.03, 0.97, colors="#bdbdbd", linewidth=1)

    n = len(labels); top = 0.85; row_h = 0.12 if n <= 4 else 0.09
    for i, (lab, p, c) in enumerate(zip(labels, pct, colors)):
        y = top - i * row_h
        ax_leg.scatter(0.07, y, s=220, color=c)
        ax_leg.text(0.12, y, str(lab), fontsize=12, va='center')
        ax_leg.text(0.95, y, format_pct2(p), fontsize=12, va='center', ha='right')

    if show_total:
        ax_leg.hlines(top - n * row_h - 0.03, 0.03, 0.97, colors="#bdbdbd", linewidth=1)
        ax_leg.text(0.12, top - n * row_h - 0.10, total_label, fontsize=12, va='center')
        ax_leg.text(0.95, top - n * row_h - 0.10, "100.00%", fontsize=12, va='center', ha='right')

    fig.patch.set_facecolor("#fcfcfc")
    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight', dpi=dpi)
    plt.close()

# ====== åœ–è¡¨ï¼šå‹•æ…‹ Y è»¸å·¥å…·å‡½å¼ ======

def dynamic_ylim_from_values(vals):
    m = int(max(vals)) if vals else 0
    if m <= 0:
        return 8
    target = m * 1.15
    exp = math.floor(math.log10(target))
    base = target / (10 ** exp)
    if base <= 1:
        nice = 1
    elif base <= 2:
        nice = 2
    elif base <= 5:
        nice = 5
    else:
        nice = 10
    return int(nice * (10 ** exp))

# ç›´æ¥ç”¨è¨ˆæ•¸ç¹ªåœ–ï¼ˆå…è¼‰å…¥æ•´æª”ï¼‰

def draw_binary_charts_from_counts(count_normal: int, count_attack: int, output_pie: str, output_bar: str):
    from matplotlib.ticker import MaxNLocator, FuncFormatter
    labels = ['æ­£å¸¸æµé‡', 'æ”»æ“Šæµé‡']
    colors = ["#04ff11", "#FF0000"]
    values = [int(count_normal), int(count_attack)]
    draw_pie_with_side_legend(values, labels, colors, output_pie, title="æ”»æ“Šèˆ‡æ­£å¸¸æµé‡æ¯”ä¾‹ï¼ˆäºŒå…ƒï¼‰", decimals=2)

    plt.figure(figsize=(7, 5))
    ax = plt.gca()
    y_top = dynamic_ylim_from_values(values)
    plt.bar(labels, values, edgecolor="#333", width=0.6)
    ax.set_ylim(0, y_top)
    ax.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
    ax.yaxis.grid(True, linewidth=1, alpha=0.25)
    for spine in ['top', 'right']:
        ax.spines[spine].set_visible(False)
    for idx, v in enumerate(values):
        y_lab = min(v + y_top * 0.05, y_top * 0.98)
        ax.text(idx, y_lab, f"{int(v):,}", ha='center', va='bottom', fontsize=13)
    ax.set_xlabel('æµé‡é¡å‹', fontsize=15, labelpad=10)
    ax.set_ylabel('æ•¸é‡ï¼ˆç­†ï¼‰', fontsize=15, labelpad=10)
    ax.set_title('æ”»æ“Šèˆ‡æ­£å¸¸æµé‡æ•¸é‡åˆ†å¸ƒï¼ˆäºŒå…ƒï¼‰', fontsize=18, pad=18)
    plt.tight_layout(); plt.savefig(output_bar, bbox_inches='tight'); plt.close()


def draw_multiclass_charts_from_counts(sev_counts: dict[int, int], output_pie: str, output_bar: str):
    from matplotlib.ticker import MaxNLocator, FuncFormatter
    severity_map = {1: 'å±éšª', 2: 'é«˜', 3: 'ä¸­', 4: 'ä½'}
    show_levels = [1, 2, 3, 4]
    sev_labels = [severity_map[i] for i in show_levels]
    colors_sev = ["#d32f2f", "#f57c08", "#fbc02d", "#04ff11"]
    vals = [int(sev_counts.get(i, 0)) for i in show_levels]

    draw_pie_with_side_legend(vals, sev_labels, colors_sev, output_pie, title="Severity åˆ†å¸ƒï¼ˆåƒ…é‡å°æ”»æ“Šæµé‡ï¼‰", decimals=2)

    plt.figure(figsize=(7, 5))
    ax = plt.gca(); ax.yaxis.set_major_locator(MaxNLocator(integer=True))
    y_top = dynamic_ylim_from_values(vals)
    ax.set_ylim(0, y_top)
    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
    plt.bar(sev_labels, vals, edgecolor="#333", width=0.6)
    for idx, v in enumerate(vals):
        y_lab = min(v + y_top * 0.05, y_top * 0.98)
        plt.text(idx, y_lab, f"{int(v):,}", ha='center', va='bottom', fontsize=13)
    plt.xlabel('Severity ç­‰ç´šï¼ˆ4ç‚ºæœ€ä½ï¼Œ1ç‚ºæœ€é«˜ï¼‰', fontsize=15, labelpad=10)
    plt.ylabel('æ•¸é‡ï¼ˆç­†ï¼‰', fontsize=15, labelpad=10)
    plt.title('Severity åˆ†å¸ƒï¼ˆåƒ…é‡å°æ”»æ“Šæµé‡ï¼‰', fontsize=18, pad=20)
    plt.tight_layout(); plt.savefig(output_bar, bbox_inches='tight'); plt.close()

# ==================== ACL è¦å‰‡èˆ‡è¦†å¯«ï¼ˆå¯é¸ï¼‰ ====================

ATTEMPT_DENY_IDS = {"710003", "106023", "106021", "106016", "106015", "106014", "106006"}

def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    need_cols = [
        "event_id", "SyslogID", "Description", "raw_log", "Severity", "Datetime",
        "DestinationPort", "DestinationIP", "deny_cnt_60", "deny_u_dport_60",
        "deny_u_dip_60", "crlevel", "is_attack", "attack_type",
        "blocked", "src_if", "dst_if", "direction", "SourceIP"
    ]
    for c in need_cols:
        if c not in df.columns:
            df[c] = ""

    # ç”¨ SyslogID è£œ event_idï¼ˆåƒ…è£œç©º/NaNï¼‰
    eid_empty = df["event_id"].astype(str).str.strip().eq("") | df["event_id"].isna()
    df.loc[eid_empty, "event_id"] = df.loc[eid_empty, "SyslogID"].astype(str)

    df["is_attack"] = pd.to_numeric(df["is_attack"], errors="coerce").fillna(0).astype(int)
    df["blocked"] = pd.to_numeric(df["blocked"], errors="coerce").fillna(0).astype(int)
    return df

# ---------- ä»‹é¢/æ–¹å‘æŠ½å– ----------

def parse_interfaces_and_direction(df: pd.DataFrame) -> pd.DataFrame:
    import re
    df = _normalize_columns(df)
    text = (df["Description"].astype(str) + " " + df["raw_log"].astype(str))

    m_to = text.str.extract(r"\bto\s+([A-Za-z0-9_-]+):\d+\.\d+\.\d+\.\d+/\d+", expand=True)
    m_on = text.str.extract(r"on\s+interface\s+([A-Za-z0-9_-]+)", expand=True)

    df.loc[m_to[0].notna(), "dst_if"] = m_to[0]
    df.loc[m_on[0].notna() & df["dst_if"].eq(""), "dst_if"] = m_on[0]

    def infer_dir(row):
        if row["dst_if"]:
            txt = f"{row['Description']} {row['raw_log']}".lower()
            try:
                ip = ipaddress.ip_address(str(row["SourceIP"]))
                is_private = ip.is_private
            except Exception:
                is_private = False
            src_if = "outside" if (("outside" in txt) or (not is_private)) else "inside"
            return f"{src_if}->{row['dst_if']}"
        return ""
    df["direction"] = df.apply(infer_dir, axis=1)
    return df

# ---------- èªæ„åµæ¸¬ï¼ˆé¸ç”¨ï¼‰ ----------

def semantic_attack_labels(df: pd.DataFrame) -> pd.DataFrame:
    df = _normalize_columns(df)
    eid = df["event_id"].astype(str).str.extract(r"(\d+)")[0].fillna("")
    mask_acl = eid.isin(ATTEMPT_DENY_IDS)

    low = (df["Description"].astype(str) + " " + df["raw_log"].astype(str)).str.lower()
    m_spoof = low.str.contains("ip spoof", na=False)
    m_no_conn = low.str.contains("no connection", na=False)
    m_deny = (
        low.str.contains(r"asa-\d+-710003", na=False) |
        low.str.contains("access denied", na=False)   |
        low.str.contains(r"\bdeny\b", na=False)       |
        low.str.contains(r"\bdenied\b", na=False)
    )

    mask_attack = mask_acl | m_deny | m_spoof | m_no_conn
    df.loc[mask_attack, "is_attack"] = 1
    df.loc[mask_attack, "blocked"] = 1

    df.loc[m_spoof, "attack_type"] = df["attack_type"].where(df["attack_type"].ne(""), "ip_spoof")
    df.loc[m_no_conn, "attack_type"] = df["attack_type"].where(df["attack_type"].ne(""), "out_of_state_tcp")
    df.loc[mask_acl | m_deny, "attack_type"] = df["attack_type"].where(df["attack_type"].ne(""), "deny_by_acl")
    return df


def add_acl_deny_window_stats(df: pd.DataFrame, window_sec: int = 60) -> pd.DataFrame:
    import numpy as np
    df = _normalize_columns(df)
    if "Datetime" not in df.columns or "event_id" not in df.columns:
        df["deny_cnt_60"] = 0; df["deny_u_dport_60"] = 0; df["deny_u_dip_60"] = 0
        return df

    mask = df["event_id"].astype(str).isin(ATTEMPT_DENY_IDS)
    if not mask.any():
        df["deny_cnt_60"] = 0; df["deny_u_dport_60"] = 0; df["deny_u_dip_60"] = 0
        return df

    d = df.copy()
    d["Datetime"] = pd.to_datetime(d["Datetime"], errors="coerce")
    d = d.sort_values("Datetime")

    d["deny_cnt_60"] = 0; d["deny_u_dport_60"] = 0; d["deny_u_dip_60"] = 0

    idxs = d[mask].index
    for idx in idxs:
        t = d.at[idx, "Datetime"]
        if pd.isna(t):
            continue
        eid = str(d.at[idx, "event_id"])
        window_start = t - pd.Timedelta(seconds=window_sec)
        win = d[(d["Datetime"] >= window_start) & (d["Datetime"] <= t) & (d["event_id"].astype(str) == eid)]
        d.at[idx, "deny_cnt_60"] = len(win)
        d.at[idx, "deny_u_dport_60"] = win["DestinationPort"].nunique() if "DestinationPort" in win.columns else 0
        d.at[idx, "deny_u_dip_60"] = win["DestinationIP"].nunique() if "DestinationIP" in win.columns else 0

    for c in ["deny_cnt_60", "deny_u_dport_60", "deny_u_dip_60"]:
        df[c] = d[c]
    return df


def overlay_acl_deny_rules(df: pd.DataFrame) -> pd.DataFrame:
    df = _normalize_columns(df.copy())
    mask = df["event_id"].astype(str).str.extract(r"(\d+)")[0].isin(ATTEMPT_DENY_IDS)
    if "is_attack" not in df.columns:
        df["is_attack"] = 0
    if "attack_type" not in df.columns:
        df["attack_type"] = ""
    df.loc[mask, "is_attack"] = 1
    df.loc[mask, "attack_type"] = df["attack_type"].where(~mask, "deny_by_acl")
    df.loc[mask, "blocked"] = 1
    return df


def overlay_acl_deny_scoring(df: pd.DataFrame) -> pd.DataFrame:
    import numpy as np
    df = _normalize_columns(df.copy())
    mask = df["event_id"].astype(str).isin(ATTEMPT_DENY_IDS)
    if "deny_cnt_60" not in df.columns:
        df.loc[mask, "crlevel"] = 2
        return df

    z = np.zeros(len(df))
    hi = mask & (
        (pd.to_numeric(df["deny_cnt_60"], errors="coerce").fillna(0) >= 10) |
        (pd.to_numeric(df.get("deny_u_dport_60", pd.Series(z)), errors="coerce").fillna(0) >= 5) |
        (pd.to_numeric(df.get("deny_u_dip_60",   pd.Series(z)), errors="coerce").fillna(0) >= 3)
    )
    med = mask & (~hi) & (
        (pd.to_numeric(df["deny_cnt_60"], errors="coerce").fillna(0).between(4, 9, inclusive="both")) |
        (pd.to_numeric(df.get("deny_u_dport_60", pd.Series(z)), errors="coerce").fillna(0).between(2, 4, inclusive="both"))
    )
    low = mask & (~hi) & (~med)

    df.loc[hi,  "crlevel"] = 1
    df.loc[med, "crlevel"] = 2
    df.loc[low, "crlevel"] = 3
    return df


def apply_acl_deny_overlay(df_pred: pd.DataFrame, window_sec: int = 60) -> pd.DataFrame:
    df_pred = add_acl_deny_window_stats(df_pred, window_sec=window_sec)
    df_pred = overlay_acl_deny_rules(df_pred)
    df_pred = overlay_acl_deny_scoring(df_pred)
    return df_pred

# =============== ç´¯ç©è¼¸å‡º/åœ–è¡¨ï¼ˆéé˜»å¡ç‰ˆï¼‰ ===============

# ä»¥ã€Œé™„åŠ å¯«å…¥ + chunk çµ±è¨ˆã€é¿å…è¼‰å…¥æ•´æª”é€ æˆå¡é “

def save_all_results_and_redraw(
    df_bin: pd.DataFrame,
    batch_id: int,
    all_results_path: str,
    binary_pie: str,
    binary_bar: str,
    overwrite: bool = False,   # åƒæ•¸ä¿ç•™ä½†ä¸ä½¿ç”¨ï¼ˆå›ºå®šé™„åŠ ï¼‰
    dedupe: bool = False,      # å¤§æª”å»é‡å¾ˆåƒè¨˜æ†¶é«”ï¼Œé è¨­é—œé–‰
):
    df_save = df_bin.copy()
    df_save["batch_id"] = batch_id

    # ç›´æ¥é™„åŠ ï¼ˆé¿å…å°‡èˆŠæª”æ•´å€‹è¼‰å…¥è¨˜æ†¶é«”ï¼‰
    write_header = not os.path.exists(all_results_path)
    with open(all_results_path, "a", newline='', encoding="utf-8") as allf:
        df_save.to_csv(allf, header=write_header, index=False)

    # å¦‚éœ€å»é‡ï¼Œåƒ…é‡å°æœ¬æ‰¹è³‡æ–™èˆ‡æœ€è¿‘ N è¡Œåšæ¥µå°ç¯„åœè™•ç†ï¼ˆæ­¤è™•ç•¥ï¼Œä¿æŒéé˜»å¡ï¼‰

    # ä»¥ chunk è®€å–çµ±è¨ˆï¼ˆåªè®€ is_attack æ¬„ï¼‰
    cnt0, cnt1 = 0, 0
    try:
        for ch in pd.read_csv(all_results_path, usecols=["is_attack"], chunksize=200_000):
            vc = ch["is_attack"].value_counts()
            cnt0 += int(vc.get(0, 0))
            cnt1 += int(vc.get(1, 0))
    except Exception:
        pass

    # ä¾è¨ˆæ•¸ç•«åœ–
    draw_binary_charts_from_counts(cnt0, cnt1, binary_pie, binary_bar)

    return {"count_all": cnt0 + cnt1, "count_normal": cnt0, "count_attack": cnt1}


def save_all_multiclass_results_and_redraw(
    df_attack_with_sev: pd.DataFrame,
    batch_id: int,
    all_multi_path: str,
    multi_all_pie: str,
    multi_all_bar: str,
    overwrite: bool = False,  # ä¿ç•™ä½†ä¸ä½¿ç”¨ï¼ˆå›ºå®šé™„åŠ ï¼‰
    dedupe: bool = False,     # é è¨­é—œé–‰ï¼Œé¿å…å¡é “
):
    """å°‡æœ¬æ‰¹æ”»æ“Š(å« Severity)å¯«å…¥ all_multiclass_results.csvï¼Œä¸¦ç”¨ chunk çµ±è¨ˆé‡ç¹ªåœ–è¡¨ã€‚"""
    df_save = df_attack_with_sev.copy()
    df_save["batch_id"] = batch_id

    write_header = not os.path.exists(all_multi_path)
    with open(all_multi_path, "a", newline='', encoding="utf-8") as allf:
        df_save.to_csv(allf, header=write_header, index=False)

    # ä»¥ chunk è®€å– Severity çµ±è¨ˆ
    sev_counts = {1: 0, 2: 0, 3: 0, 4: 0}
    try:
        for ch in pd.read_csv(all_multi_path, usecols=["Severity"], chunksize=200_000):
            vc = pd.to_numeric(ch["Severity"], errors="coerce").value_counts()
            for k in (1, 2, 3, 4):
                sev_counts[k] += int(vc.get(k, 0))
    except Exception:
        pass

    draw_multiclass_charts_from_counts(sev_counts, multi_all_pie, multi_all_bar)

    return {"count_all": sum(sev_counts.values()), "severity_distribution": sev_counts}

# =============== STEP1 ===============

def step1_process_logs(raw_log_path, step1_out_path, unique_out_json, batch_id, show_progress=True):
    import gzip, io
    import chardet

    STANDARD_COLUMNS = [
        "batch_id", "Datetime", "SyslogID", "Severity", "SourceIP", "SourcePort",
        "DestinationIP", "DestinationPort", "Duration", "Bytes",
        "Protocol", "Action", "Description", "raw_log"
    ]

    # é˜²è¨˜æ†¶é«”æš´è¡ï¼šæ¯æ¬„å”¯ä¸€å€¼åªè¨˜åˆ°é€™å€‹ä¸Šé™
    MAX_UNIQUES_PER_COL = 1000
    unique_vals = {col: set() for col in STANDARD_COLUMNS}

    def detect_encoding_safely(file_path, sample_bytes=200_000):
        try:
            if str(file_path).lower().endswith(".gz"):
                with gzip.open(file_path, "rb") as f:
                    raw = f.read(sample_bytes)
            else:
                with open(file_path, "rb") as f:
                    raw = f.read(sample_bytes)
            enc = chardet.detect(raw).get("encoding") or "utf-8"
            return enc
        except Exception:
            return "utf-8"

    def count_lines_binary(file_path):
        bufsize = 1024 * 1024
        total = 0
        if str(file_path).lower().endswith(".gz"):
            with gzip.open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(bufsize), b""):
                    total += chunk.count(b"\n")
        else:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(bufsize), b""):
                    total += chunk.count(b"\n")
        return max(total, 1)

    def open_text_stream(file_path, encoding):
        if str(file_path).lower().endswith(".gz"):
            g = gzip.open(file_path, "rb")
            return io.TextIOWrapper(g, encoding=encoding, errors="replace", newline="")
        else:
            return open(file_path, "r", encoding=encoding, errors="replace", newline="")

    print(f"{Fore.CYAN}{Style.BRIGHT}STEP1ï¼šé–‹å§‹è³‡æ–™æ¸…æ´—èˆ‡å”¯ä¸€å€¼çµ±è¨ˆ")
    encoding = detect_encoding_safely(raw_log_path)
    total_lines = count_lines_binary(raw_log_path)

    processed_count = 0
    f = open_text_stream(raw_log_path, encoding)
    try:
        with f as tf, open(step1_out_path, "w", newline='', encoding="utf-8") as out_f:
            reader = csv.DictReader(tf)
            writer = csv.DictWriter(out_f, fieldnames=STANDARD_COLUMNS)
            writer.writeheader()
            pbar_total = max(total_lines - 1, 0)
            pbar = tqdm(reader, total=pbar_total, desc=f"{Fore.CYAN}æ¸…æ´—é€²åº¦", disable=not show_progress)

            for row in pbar:
                record = {col: row.get(col, "") for col in STANDARD_COLUMNS if col != "batch_id" and col != "raw_log"}
                record["batch_id"] = batch_id
                record["raw_log"] = json.dumps(row, ensure_ascii=False)
                for col in STANDARD_COLUMNS:
                    if col not in record:
                        record[col] = "unknown"
                    # é™åˆ¶å”¯ä¸€å€¼é›†åˆå¤§å°ï¼Œé¿å…çˆ†è¨˜æ†¶é«”
                    if len(unique_vals[col]) < MAX_UNIQUES_PER_COL:
                        unique_vals[col].add(record[col])
                writer.writerow(record)
                processed_count += 1
        print(f"{Fore.GREEN}{Style.BRIGHT}STEP1 çµæŸï¼Œå…±è™•ç† {processed_count} ç­†è³‡æ–™ã€‚")
    finally:
        try:
            f.close()
        except Exception:
            pass

    unique_json = {k: sorted([str(x) for x in v]) for k, v in unique_vals.items()}
    with open(unique_out_json, "w", encoding="utf-8") as fjson:
        json.dump(unique_json, fjson, ensure_ascii=False, indent=4)
    print(f"{Fore.GREEN}{Style.BRIGHT}STEP1 å®Œæˆï¼šå·²è¼¸å‡º {step1_out_path} åŠ {unique_out_json}")

# =============== STEP2 ===============

def step2_preprocess_data(step1_out_path, step2_out_path, unique_json, show_progress=True):
    with open(unique_json, "r", encoding="utf-8") as f:
        unique_vals = json.load(f)
    CATEGORICAL_MAPPINGS = {
        "Protocol": {"http": 1, "https": 2, "icmp": 3, "tcp": 4, "udp": 5, "scan": 6, "flood": 7, "other": 8, "unknown": 0, "nan": -1},
        "Action": {"built": 1, "teardown": 2, "deny": 3, "drop": 4, "login": 5, "other": 6, "unknown": 0, "nan": -1}
    }
    print(f"{Fore.CYAN}{Style.BRIGHT}STEP2ï¼šé–‹å§‹æ¬„ä½æ¨™æº–åŒ–èˆ‡æ˜ å°„")

    column_order = [
        "batch_id", "Datetime", "SyslogID", "Severity", "is_attack", "SourceIP", "SourcePort",
        "DestinationIP", "DestinationPort", "Duration", "Bytes", "Protocol",
        "Action", "Description", "raw_log"
    ]
    chunks = []
    numeric_cols = ["SourcePort", "DestinationPort", "Duration", "Bytes"]

    for chunk in tqdm(pd.read_csv(step1_out_path, chunksize=50_000),
                      desc=f"{Fore.CYAN}é è™•ç†é€²åº¦", disable=not show_progress):
        for col, mapping in CATEGORICAL_MAPPINGS.items():
            if col in chunk.columns:
                chunk[col] = chunk[col].astype(str).str.lower().map(mapping).fillna(-1).astype(int)
        chunk["is_attack"] = 0
        for col in numeric_cols:
            if col in chunk.columns:
                chunk[col] = pd.to_numeric(chunk[col], errors="coerce").fillna(0).astype(int)
        for col in column_order:
            if col not in chunk.columns:
                chunk[col] = ""
        chunk = chunk[column_order]
        chunks.append(chunk)

    df = pd.concat(chunks, ignore_index=True)
    before = len(df)
    df.drop_duplicates(inplace=True)
    after = len(df)
    print(f"{Fore.GREEN}å»é™¤é‡è¤‡è³‡æ–™ {before-after} ç­†ï¼Œå…±ä¿ç•™ {after} ç­†")
    df.to_csv(step2_out_path, index=False, encoding="utf-8")
    print(f"{Fore.GREEN}{Style.BRIGHT}STEP2 å®Œæˆï¼šå·²è¼¸å‡º {step2_out_path}")

# ========== STEP3-1ï¼šäºŒå…ƒæ¨¡å‹é æ¸¬+åœ–è¡¨ ==========

def dflare_binary_predict(input_csv, binary_model_path, output_csv, output_pie, output_bar, feat_cols=None, show_progress=True):
    from matplotlib.font_manager import FontProperties
    from matplotlib.ticker import MaxNLocator, FuncFormatter
    import joblib

    # å­—å‹è¨­å®šï¼ˆè·¨å¹³å°ï¼‰
    if os.name == "nt":
        font_path = "C:/Windows/Fonts/msjh.ttc"
    elif os.path.exists("/System/Library/Fonts/PingFang.ttc"):
        font_path = "/System/Library/Fonts/PingFang.ttc"
    else:
        font_path = "/usr/share/fonts/truetype/arphic/uming.ttc"
    font_name = FontProperties(fname=font_path).get_name()
    plt.rcParams['font.family'] = font_name
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.facecolor'] = "#fcfcfc"

    tqdm.write(f"{Fore.CYAN}{Style.BRIGHT}STEP3-1ï¼šè¼‰å…¥é è™•ç†è³‡æ–™èˆ‡äºŒå…ƒæ¨¡å‹...")
    df = pd.read_csv(input_csv, encoding="utf-8")
    bin_model = joblib.load(binary_model_path)

    if hasattr(bin_model, "feature_names_in_"):
        model_feat_cols = list(bin_model.feature_names_in_)
    elif feat_cols is not None:
        model_feat_cols = feat_cols
    else:
        raise RuntimeError("äºŒå…ƒæ¨¡å‹æœªå­˜ç‰¹å¾µåç¨±ï¼Œè«‹æ˜ç¢ºæŒ‡å®š feat_colsã€‚")
    df_model = df.reindex(columns=model_feat_cols, fill_value=-1).fillna(-1).astype(int)

    tqdm.write(f"{Fore.CYAN}{Style.BRIGHT}STEP3-1ï¼šäºŒå…ƒæ¨¡å‹é æ¸¬èˆ‡åœ–è¡¨ç”¢ç”Ÿä¸­...")
    df['is_attack'] = bin_model.predict(df_model)
    df.to_csv(output_csv, index=False, encoding="utf-8")
    tqdm.write(f"{Fore.GREEN}{Style.BRIGHT}STEP3-1ï¼šäºŒå…ƒåˆ¤æ–·å·²è¼¸å‡º {output_csv}")

    is_attack_dist = df['is_attack'].value_counts().sort_index().reindex([0, 1], fill_value=0)
    values = [int(is_attack_dist.get(0, 0)), int(is_attack_dist.get(1, 0))]
    labels = ['æ­£å¸¸æµé‡', 'æ”»æ“Šæµé‡']
    colors = ["#04ff11", "#FF0000"]

    draw_pie_with_side_legend(values, labels, colors, output_pie, title="æ”»æ“Šèˆ‡æ­£å¸¸æµé‡æ¯”ä¾‹ï¼ˆäºŒå…ƒï¼‰", decimals=2)

    plt.figure(figsize=(7, 5))
    ax = plt.gca()
    vals = [int(is_attack_dist.loc[0]), int(is_attack_dist.loc[1])]
    y_top = dynamic_ylim_from_values(vals)
    plt.bar(labels, vals, edgecolor="#333", width=0.6)
    ax.set_ylim(0, y_top)
    ax.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
    ax.yaxis.grid(True, linewidth=1, alpha=0.25)
    for spine in ['top', 'right']:
        ax.spines[spine].set_visible(False)
    for idx, v in enumerate(vals):
        y_lab = min(v + y_top * 0.05, y_top * 0.98)
        ax.text(idx, y_lab, f"{int(v):,}", ha='center', va='bottom', fontsize=13)
    ax.set_xlabel('æµé‡é¡å‹', fontsize=15, labelpad=10)
    ax.set_ylabel('æ•¸é‡ï¼ˆç­†ï¼‰', fontsize=15, labelpad=10)
    ax.set_title('æ”»æ“Šèˆ‡æ­£å¸¸æµé‡æ•¸é‡åˆ†å¸ƒï¼ˆäºŒå…ƒï¼‰', fontsize=18, pad=18)
    plt.tight_layout(); plt.savefig(output_bar, bbox_inches='tight'); plt.close()

    return {
        "output_csv": output_csv,
        "output_pie": output_pie,
        "output_bar": output_bar,
        "is_attack_distribution": is_attack_dist.to_dict(),
        "count_all": int(df.shape[0]),
        "count_attack": int(is_attack_dist[1]),
        "count_normal": int(is_attack_dist[0]),
    }, df

# ========== STEP3-2ï¼šå¤šå…ƒæ¨¡å‹é æ¸¬+åœ–è¡¨ ==========

def dflare_multiclass_predict(df_attack, multiclass_model_path, output_csv, output_pie, output_bar, feat_cols=None, show_progress=True):
    from matplotlib.font_manager import FontProperties
    from matplotlib.ticker import MaxNLocator, FuncFormatter
    import joblib

    if os.name == "nt":
        font_path = "C:/Windows/Fonts/msjh.ttc"
    elif os.path.exists("/System/Library/Fonts/PingFang.ttc"):
        font_path = "/System/Library/Fonts/PingFang.ttc"
    else:
        font_path = "/usr/share/fonts/truetype/arphic/uming.ttc"
    font_name = FontProperties(fname=font_path).get_name()
    plt.rcParams['font.family'] = font_name
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.facecolor'] = "#fcfcfc"

    severity_map = {1: 'å±éšª', 2: 'é«˜', 3: 'ä¸­', 4: 'ä½'}
    show_levels = [1, 2, 3, 4]
    sev_labels = [severity_map[i] for i in show_levels]
    colors_sev = ["#d32f2f", "#f57c08", "#fbc02d", "#04ff11"]

    tqdm.write(f"{Fore.CYAN}{Style.BRIGHT}STEP3-2ï¼šè¼‰å…¥æ”»æ“Šæµé‡èˆ‡å¤šå…ƒæ¨¡å‹...")
    mul_model = joblib.load(multiclass_model_path)
    if hasattr(mul_model, "feature_names_in_"):
        model_feat_cols = list(mul_model.feature_names_in_)
    elif feat_cols is not None:
        model_feat_cols = feat_cols
    else:
        raise RuntimeError("å¤šå…ƒæ¨¡å‹æœªå­˜ç‰¹å¾µåç¨±ï¼Œè«‹æ˜ç¢ºæŒ‡å®š feat_colsã€‚")
    df_model = df_attack.reindex(columns=model_feat_cols, fill_value=-1).fillna(-1).astype(int)

    tqdm.write(f"{Fore.CYAN}{Style.BRIGHT}STEP3-2ï¼šå¤šå…ƒæ¨¡å‹åˆ†ç´šèˆ‡åœ–è¡¨ç”¢ç”Ÿä¸­...")
    df_attack['Severity'] = mul_model.predict(df_model)
    df_attack.to_csv(output_csv, index=False, encoding="utf-8")
    tqdm.write(f"{Fore.GREEN}{Style.BRIGHT}STEP3-2ï¼šå¤šå…ƒåˆ†ç´šå·²è¼¸å‡º {output_csv}")

    sev_dist = df_attack['Severity'].value_counts().sort_index().reindex(show_levels, fill_value=0)
    vals = [int(sev_dist.loc[i]) for i in show_levels]

    draw_pie_with_side_legend(vals, sev_labels, colors_sev, output_pie, title="Severity åˆ†å¸ƒï¼ˆåƒ…é‡å°æ”»æ“Šæµé‡ï¼‰", decimals=2)

    plt.figure(figsize=(7, 5))
    ax = plt.gca(); ax.yaxis.set_major_locator(MaxNLocator(integer=True))
    y_top = dynamic_ylim_from_values(vals)
    ax.set_ylim(0, y_top)
    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
    plt.bar(sev_labels, vals, edgecolor="#333", width=0.6)
    for idx, v in enumerate(vals):
        y_lab = min(v + y_top * 0.05, y_top * 0.98)
        plt.text(idx, y_lab, f"{int(v):,}", ha='center', va='bottom', fontsize=13)
    plt.xlabel('Severity ç­‰ç´šï¼ˆ4ç‚ºæœ€ä½ï¼Œ1ç‚ºæœ€é«˜ï¼‰', fontsize=15, labelpad=10)
    plt.ylabel('æ•¸é‡ï¼ˆç­†ï¼‰', fontsize=15, labelpad=10)
    plt.title('Severity åˆ†å¸ƒï¼ˆåƒ…é‡å°æ”»æ“Šæµé‡ï¼‰', fontsize=18, pad=20)
    plt.tight_layout(); plt.savefig(output_bar, bbox_inches='tight'); plt.close()

    return {
        "output_csv": output_csv,
        "output_pie": output_pie,
        "output_bar": output_bar,
        "severity_distribution": sev_dist.to_dict(),
        "count_all": int(df_attack.shape[0]),
        "message": "å¤šå…ƒåˆ†ç´šçµæœå·²ç”¢ç”Ÿ"
    }, df_attack

# ========== PIPELINE ==========

def dflare_sys_full_pipeline(
    raw_log_path, binary_model_path, multiclass_model_path, output_dir,
    all_results_csv="all_results.csv",
    all_multiclass_csv="all_multiclass_results.csv",
    bin_feat_cols=None, multi_feat_cols=None, show_progress=True,
    force_all_attack=False,
    overwrite_all_results=False,
    dedupe_all_results=False,
    whitelist_path=None,
    discord_webhook_url: str | None = None,
    notify_dedup_sec: int = 120,
    notify_growth_ratio: float = 0.5,
    notify_update_instead: bool = True,
    notify_min_interval_sec: int = 5,
    only_once=True
) -> dict:
    os.makedirs(output_dir, exist_ok=True)

    # [1] æ‰¹æ¬¡æµæ°´è™Ÿ
    all_results_path = os.path.join(output_dir, all_results_csv)
    all_multi_path = os.path.join(output_dir, all_multiclass_csv)
    batch_id = get_next_batch_id(all_results_path)

    # [2] æš«å­˜æª”
    step1_out = os.path.join(output_dir, "processed_logs.csv")
    unique_json = os.path.join(output_dir, "log_unique_values.json")
    step2_out = os.path.join(output_dir, "preprocessed_data.csv")
    binary_csv = os.path.join(output_dir, "binary_result.csv")
    binary_pie = os.path.join(output_dir, "binary_pie.png")
    binary_bar = os.path.join(output_dir, "binary_bar.png")
    multi_csv = os.path.join(output_dir, "multiclass_result.csv")
    multi_pie = os.path.join(output_dir, "multiclass_pie.png")
    multi_bar = os.path.join(output_dir, "multiclass_bar.png")
    multi_all_pie = os.path.join(output_dir, "multiclass_pie_all.png")
    multi_all_bar = os.path.join(output_dir, "multiclass_bar_all.png")
    alerts_json = os.path.join(output_dir, "alerts.json")
    discord_state_json = os.path.join(output_dir, "discord_state.json")  # å»é‡ç‹€æ…‹

    # [3] ETL
    step1_process_logs(raw_log_path, step1_out, unique_json, batch_id, show_progress)
    step2_preprocess_data(step1_out, step2_out, unique_json, show_progress)

    # [4] äºŒå…ƒæ¨¡å‹ï¼ˆåªä¾æ¨¡å‹çµæœï¼‰
    bin_res, df_bin = dflare_binary_predict(
        input_csv=step2_out,
        binary_model_path=binary_model_path,
        output_csv=binary_csv,
        output_pie=binary_pie,
        output_bar=binary_bar,
        feat_cols=bin_feat_cols,
        show_progress=show_progress
    )

    # åªä¿ç•™æ¨¡å‹çµæœï¼›å¦‚éœ€å…¨æ”»æ“Šæ¸¬è©¦ï¼Œå¯ç”¨ force_all_attack
    if force_all_attack:
        df_bin["is_attack"] = 1

    # å…ˆå¯«å›
    df_bin.to_csv(binary_csv, index=False, encoding="utf-8")

    # [5] å¤šå…ƒåˆ†ç´šï¼ˆåƒ…æ”»æ“Šæµé‡ï¼›å®Œå…¨äº¤çµ¦æ¨¡å‹ï¼‰
    df_attack = df_bin[df_bin['is_attack'] == 1].copy()
    if df_attack.empty:
        print(f"{Fore.YELLOW}{Style.BRIGHT}æœ¬æ‰¹è³‡æ–™ç„¡æ”»æ“Šæµé‡ï¼ˆis_attack=1ï¼‰ï¼Œè·³éå¤šå…ƒåˆ†ç´šã€‚")
        agg_stats = save_all_results_and_redraw(
            df_bin, batch_id, all_results_path, binary_pie, binary_bar,
            overwrite_all_results, dedupe_all_results,
        )
        bin_res["is_attack_distribution"] = {0: agg_stats.get("count_normal", 0), 1: agg_stats.get("count_attack", 0)}
        bin_res["count_all"] = int(agg_stats.get("count_all", 0))
        bin_res["count_attack"] = int(agg_stats.get("count_attack", 0))
        bin_res["count_normal"] = int(agg_stats.get("count_normal", 0))
        build_alerts(df_bin, alerts_json)

        # æ²’æœ‰æ”»æ“Šå°±ä¸æ¨æ’­
        return {
            "batch_id": batch_id,
            "binary": bin_res,
            "multiclass": None,
            "binary_output_csv": binary_csv,
            "binary_output_pie": binary_pie,
            "binary_output_bar": binary_bar,
            "alerts_json": alerts_json,
            "multiclass_output_csv": None,
            "multiclass_output_pie": None,
            "multiclass_output_bar": None,
            "all_results_csv": all_results_path,
            "all_multiclass_csv": all_multi_path,
            "multiclass_all_pie": multi_all_pie,
            "multiclass_all_bar": multi_all_bar,
            "discord_notify": {"action": "skip", "message_id": None, "reason": "no_attack"},
            "message": "æœ¬æ‰¹ç„¡æ”»æ“Šæµé‡ï¼Œå·²è·³éå¤šå…ƒåˆ†ç´š",
        }

    # æœ‰æ”»æ“Šæµé‡æ‰é€²è¡Œå¤šå…ƒåˆ†ç´šï¼ˆç´”æ¨¡å‹ï¼‰
    multi_res, df_attack = dflare_multiclass_predict(
        df_attack=df_attack,
        multiclass_model_path=multiclass_model_path,
        output_csv=multi_csv,
        output_pie=multi_pie,
        output_bar=multi_bar,
        feat_cols=multi_feat_cols,
        show_progress=show_progress
    )

    # æŠŠ Severity å›å¯«åˆ° df_binï¼ˆåƒ… is_attack==1ï¼‰
    df_bin.loc[df_attack.index, "Severity"] = df_attack["Severity"].values
    df_bin.to_csv(binary_csv, index=False, encoding="utf-8")

    # [6] ç´¯ç©è¼¸å‡ºèˆ‡é‡ç¹ªï¼ˆéé˜»å¡ç‰ˆï¼‰
    agg_stats = save_all_results_and_redraw(
        df_bin, batch_id, all_results_path, binary_pie, binary_bar,
        overwrite_all_results, dedupe_all_results,
    )
    bin_res["is_attack_distribution"] = {0: agg_stats.get("count_normal", 0), 1: agg_stats.get("count_attack", 0)}
    bin_res["count_all"] = int(agg_stats.get("count_all", 0))
    bin_res["count_attack"] = int(agg_stats.get("count_attack", 0))
    bin_res["count_normal"] = int(agg_stats.get("count_normal", 0))

    multi_stats = save_all_multiclass_results_and_redraw(
        df_attack_with_sev=df_attack,
        batch_id=batch_id,
        all_multi_path=all_multi_path,
        multi_all_pie=multi_all_pie,
        multi_all_bar=multi_all_bar,
        overwrite=overwrite_all_results,
        dedupe=dedupe_all_results,
    )
    multi_res["severity_distribution"] = {int(k): int(v) for k, v in multi_stats.get("severity_distribution", {}).items()}
    multi_res["count_all"] = int(multi_stats.get("count_all", 0))

    # [7] å‘Šè­¦å½™æ•´
    build_alerts(df_bin, alerts_json)

    # [8] Discord å»é‡æ¨æ’­ï¼ˆå¯é¸ï¼›éé˜»å¡ç­–ç•¥ï¼‰
    discord_info = {"action": "skip", "message_id": None}
    if discord_webhook_url:
        try:
            discord_info = notify_discord_dedup(
                df_attack=df_attack,
                webhook_url=discord_webhook_url,
                state_path=discord_state_json,
                batch_id=batch_id,
                dedup_sec=notify_dedup_sec,
                growth_ratio=notify_growth_ratio,
                update_instead=True,
                only_once_update=True,
            )
        except Exception as e:
            print(f"{Fore.YELLOW}Discord æ¨æ’­ä¾‹å¤–ï¼š{e}")

    return {
        "batch_id": batch_id,
        "binary": bin_res,
        "multiclass": multi_res,
        "binary_output_csv": binary_csv,
        "binary_output_pie": binary_pie,
        "binary_output_bar": binary_bar,
        "alerts_json": alerts_json,
        "multiclass_output_csv": multi_csv,
        "multiclass_output_pie": multi_pie,
        "multiclass_output_bar": multi_bar,
        "all_results_csv": all_results_path,
        "all_multiclass_csv": all_multi_path,
        "multiclass_all_pie": multi_all_pie,
        "multiclass_all_bar": multi_all_bar,
        "discord_notify": discord_info,
    }

# ---------- å‘Šè­¦å½™æ•´ ----------

def build_alerts(df: pd.DataFrame, out_json: str):
    df = _normalize_columns(df)
    out = {}
    vc = df.loc[df["is_attack"] == 1, "crlevel"].value_counts(dropna=False).to_dict()
    out["attack_crlevel_distribution"] = {str(k): int(v) for k, v in vc.items()}
    vt = df.loc[df["is_attack"] == 1, "attack_type"].value_counts().to_dict()
    out["attack_type_distribution"] = {str(k): int(v) for k, v in vt.items()}
    vd = df.loc[df["is_attack"] == 1, "direction"].value_counts().head(10).to_dict()
    out["top_directions"] = vd
    # æ–°å¢ï¼šSeverity åˆ†å¸ƒï¼ˆåƒ…æ”»æ“Šæµé‡ï¼Œè‹¥æœ‰ï¼‰
    if "Severity" in df.columns:
        vs = df.loc[df["is_attack"] == 1, "Severity"].value_counts().sort_index().to_dict()
        out["severity_distribution"] = {str(k): int(v) for k, v in vs.items()}
    # æ¬„ä½å…¼å®¹
    src_col = "SourceIP" if "SourceIP" in df.columns else ("src_ip" if "src_ip" in df.columns else None)
    if src_col:
        vs = df.loc[df["is_attack"] == 1, src_col].value_counts().head(10).to_dict()
        out["top_source_ip"] = {str(k): int(v) for k, v in vs.items()}
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

# ==== å–å¾—æ–°çš„ batch_id ====

def get_next_batch_id(all_results_path):
    if not os.path.exists(all_results_path):
        return 1
    try:
        df = pd.read_csv(all_results_path, usecols=["batch_id"])  # åªè®€ä¸€æ¬„ï¼Œé¿å…å¡é “
        if "batch_id" in df.columns and not df.empty:
            return int(df["batch_id"].max()) + 1
        else:
            return 1
    except Exception:
        return 1

# ==================== CLI ====================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="D-FLARE SYS å…¨æµç¨‹è‡ªå‹•æ‰¹æ¬¡å·¥å…·ï¼ˆCisco ASA 5506-Xï½œéé˜»å¡ç©©å®šç‰ˆï¼‰"
    )
    # åŸºæœ¬ I/Oï¼ˆå¿…å¡«ï¼Œé¿å…äº’å‹•å¼é˜»å¡ï¼‰
    parser.add_argument('--raw_log', type=str, required=True, help='åŸå§‹ log æª”ï¼ˆcsv/txt/gzï¼‰')
    parser.add_argument('--bin_model', type=str, required=True, help='äºŒå…ƒæ¨¡å‹ .pkl')
    parser.add_argument('--multi_model', type=str, required=True, help='å¤šå…ƒæ¨¡å‹ .pkl')
    parser.add_argument('--output_dir', type=str, required=True, help='è¼¸å‡ºç›®éŒ„')

    # é¸é …
    parser.add_argument('--no_progress', action='store_true', help='é—œé–‰é€²åº¦åˆ—ï¼ˆå»ºè­°å¤§é‡è³‡æ–™æ™‚é–‹å•Ÿï¼‰')
    parser.add_argument('--force_all_attack', action='store_true', help='å°‡æœ¬æ‰¹å…¨æ•¸è¦–ç‚ºæ”»æ“Šï¼ˆæ¸¬è©¦ç”¨ï¼‰')
    parser.add_argument('--overwrite_all_results', action='store_true', help='è¦†å¯«ç´¯ç© all_results.csvï¼ˆä¸å»ºè­°ï¼‰')
    parser.add_argument('--dedupe_all_results', action='store_true', help='é™„åŠ æ™‚å»é‡è¤‡ï¼ˆå¤§æª”å¯èƒ½å¡ï¼‰')
    parser.add_argument('--whitelist', type=str, help='ç™½åå–®æª”ï¼ˆä¿ç•™åƒæ•¸ï¼Œç¨‹å¼ä¸­å¯è‡ªè¡Œæ¥ï¼‰')

    # Discord å»é‡æ¨æ’­
    parser.add_argument('--discord_webhook', type=str, help='Discord Webhook URLï¼ˆå¯é¸ï¼šå•Ÿç”¨å»é‡æ¨æ’­ï¼‰')
    parser.add_argument('--notify_dedup_sec', type=int, default=120, help='å»é‡è¦–çª—ç§’æ•¸ï¼ˆé è¨­ 120ï¼‰')
    parser.add_argument('--notify_growth_ratio', type=float, default=0.5, help='è¦–çª—å…§æ•¸é‡æˆé•·æ¯”ä¾‹é–€æª»ï¼ˆé è¨­ 0.5ï¼‰')
    parser.add_argument('--notify_update_instead', action="store_true", help='æ»¿è¶³é–€æª»æ™‚æ¡ç”¨ PATCH æ›´æ–°åŸè¨Šæ¯')
    parser.add_argument('--notify_min_interval_sec', type=int, default=5, help='åŒä¸€ç°½ç« å…©æ¬¡æ›´æ–°æœ€å°é–“éš”ç§’æ•¸ï¼ˆé è¨­ 5ï¼‰')

    args = parser.parse_args()

    show_progress = not args.no_progress
    os.makedirs(args.output_dir, exist_ok=True)

    print(f"{Fore.CYAN}{Style.BRIGHT}ğŸš¦ å•Ÿå‹• D-FLARE SYS å…¨æµç¨‹ Pipelineï¼ˆéé˜»å¡ç©©å®šç‰ˆï¼‰...")
    t0 = time.time()
    result = dflare_sys_full_pipeline(
        raw_log_path=args.raw_log,
        binary_model_path=args.bin_model,
        multiclass_model_path=args.multi_model,
        output_dir=args.output_dir,
        bin_feat_cols=None,
        multi_feat_cols=None,
        show_progress=show_progress,
        force_all_attack=args.force_all_attack,
        overwrite_all_results=args.overwrite_all_results,
        dedupe_all_results=args.dedupe_all_results,
        whitelist_path=args.whitelist,
        discord_webhook_url=args.discord_webhook,
        notify_dedup_sec=args.notify_dedup_sec,
        notify_growth_ratio=args.notify_growth_ratio,
        notify_update_instead=args.notify_update_instead,
        notify_min_interval_sec=args.notify_min_interval_sec,
    )
    print(f"{Fore.GREEN}{Style.BRIGHT}ğŸš€ å…¨æµç¨‹å®Œæˆï¼çµæœå·²è¼¸å‡ºè‡³ï¼š{args.output_dir}")
    print(json.dumps(result, ensure_ascii=False, indent=2))
    print(f"{Fore.CYAN}{Style.BRIGHT}ç¸½è€—æ™‚ï¼š{time.time() - t0:.1f} ç§’ã€‚")
